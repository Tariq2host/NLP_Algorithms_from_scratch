{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP basic challenges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Frequency Counter 📊 Challenge:\n",
    "Write a Python function that takes a text document as input and returns a dictionary containing the frequency of each word in the document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-basic approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def count_doc(doc_path):\n",
    "    vocab = {}\n",
    "    with open(doc_path, 'r', encoding='utf-8') as doc:\n",
    "        for line in doc:\n",
    "            # Remove punctuation and convert to lowercase\n",
    "            words = re.findall(r'\\b\\w+\\b', line.lower())\n",
    "            for word in words:\n",
    "                if word not in vocab:\n",
    "                    vocab[word] = 1\n",
    "                else:\n",
    "                    vocab[word] += 1\n",
    "    return vocab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2- map-reduce approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Concept de MapReduce :**\n",
    "MapReduce est un paradigme de programmation utilisé pour traiter de grandes quantités de données de manière distribuée. Il se divise en deux étapes principales :\n",
    "\n",
    "1. **Map** : \n",
    "   - Divise le travail en tâches indépendantes.\n",
    "   - Transforme les données d'entrée en paires clé-valeur `(clé, valeur)`.\n",
    "   - Exemples : Compter les mots, générer `(mot, 1)` pour chaque mot trouvé.\n",
    "\n",
    "2. **Reduce** :\n",
    "   - Regroupe les paires ayant la même clé.\n",
    "   - Applique une opération (comme une somme, une moyenne, etc.) pour réduire plusieurs valeurs associées à une clé en une seule.\n",
    "   - Exemples : Additionner les valeurs pour obtenir la fréquence totale de chaque mot.\n",
    "\n",
    "---\n",
    "\n",
    "### **Étapes de MapReduce :**\n",
    "1. **Split** :\n",
    "   - Diviser les données d'entrée en morceaux (partitions).\n",
    "   - Exemple : Un fichier texte de 1 To divisé en blocs de 64 Mo.\n",
    "\n",
    "2. **Map** :\n",
    "   - Traiter chaque morceau de manière indépendante pour générer des paires clé-valeur.\n",
    "   - Exemple : Depuis un texte, produire `(mot, 1)` pour chaque mot.\n",
    "\n",
    "3. **Shuffle and Sort** :\n",
    "   - Regrouper et trier les données par clé (toutes les occurrences de la même clé ensemble).\n",
    "   - Exemple : Regrouper tous les `(mot, 1)` par mot.\n",
    "\n",
    "4. **Reduce** :\n",
    "   - Effectuer une opération sur les groupes de clés triés.\n",
    "   - Exemple : Calculer la somme des valeurs pour chaque clé `(mot, total)`.\n",
    "\n",
    "5. **Output** :\n",
    "   - Produire les résultats finaux.\n",
    "   - Exemple : Une liste contenant chaque mot et sa fréquence.\n",
    "\n",
    "---\n",
    "\n",
    "### **Illustration avec un exemple (comptage de mots) :**\n",
    "#### Entrée :\n",
    "```\n",
    "Texte : \"Bonjour monde, bonjour Python\"\n",
    "```\n",
    "\n",
    "#### Étapes :\n",
    "1. **Map** :\n",
    "   ```\n",
    "   Bonjour -> (bonjour, 1)\n",
    "   monde -> (monde, 1)\n",
    "   bonjour -> (bonjour, 1)\n",
    "   Python -> (python, 1)\n",
    "   ```\n",
    "\n",
    "2. **Shuffle and Sort** :\n",
    "   ```\n",
    "   (bonjour, [1, 1]), (monde, [1]), (python, [1])\n",
    "   ```\n",
    "\n",
    "3. **Reduce** :\n",
    "   ```\n",
    "   (bonjour, 2), (monde, 1), (python, 1)\n",
    "   ```\n",
    "\n",
    "#### Résultat final :\n",
    "```\n",
    "{'bonjour': 2, 'monde': 1, 'python': 1}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Avantages :**\n",
    "- **Scalabilité** : Idéal pour traiter des données massives sur plusieurs machines.\n",
    "- **Parallélisme** : Les étapes Map et Reduce peuvent être parallélisées.\n",
    "\n",
    "### **Limitation** :\n",
    "- Pas adapté à tous les problèmes (par exemple, ceux nécessitant de fortes dépendances entre données)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def map_words(doc_path):\n",
    "    with open(doc_path, 'r', encoding='utf-8') as doc:\n",
    "        for line in doc:\n",
    "            words = re.findall(r'\\b\\w+\\b', line.lower())  # Clean and tokenize\n",
    "            for word in words:\n",
    "                yield (word, 1)  # Yield each word with a count of 1\n",
    "\n",
    "# Reduce function: aggregates counts for each word\n",
    "def reduce_word_counts(mapped_words):\n",
    "    word_counts = {}\n",
    "    for word, count in mapped_words:\n",
    "        if word not in word_counts:\n",
    "            word_counts[word] += 1  # Sum up counts for each word\n",
    "        else:\n",
    "            word_counts[word] += count\n",
    "    return word_counts\n",
    "\n",
    "\n",
    "# Combine Map and Reduce\n",
    "def map_reduce_word_count(doc_path):\n",
    "    mapped_words = map_words(doc_path)  # Step 1: Map\n",
    "    reduced_counts = reduce_word_counts(mapped_words)  # Step 2: Reduce\n",
    "    return reduced_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Cleaning and Tokenization 🧹 Challenge:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function to clean and tokenize a given text, removing punctuation and converting words to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello,', 'world']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = \"hello, world\"\n",
    "test.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # lower the text\n",
    "    text = text.lower()\n",
    "    # remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    # tokenize\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> re.sub(r'[^\\w\\s]', '', text) :\n",
    "\n",
    "Supprime tous les caractères qui ne sont ni alphanumériques (\\w) ni des espaces (\\s).\n",
    "\n",
    "> re.sub(r'\\s+', ' ', text).strip() :\n",
    "\n",
    "Remplace plusieurs espaces par un seul, puis enlève les espaces superflus au début ou à la fin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopword Removal 🚫 Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Develop a function that removes stopwords (commonly used words) from a given sentence or text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m stopwords\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Téléchargement des stop words (à faire une seule fois)\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Téléchargement des stop words (à faire une seule fois)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    # Définir la liste des stop words en anglais\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    # Supprimer les stop words du texte\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis 🌟 Challenge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Download required NLTK datasets\n",
    "nltk.download('movie_reviews')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load the dataset\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)]\n",
    "\n",
    "# Shuffle the dataset\n",
    "import random\n",
    "random.shuffle(documents)\n",
    "\n",
    "# Preprocess text (remove stop words and convert to lowercase)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess(words):\n",
    "    return ' '.join([word.lower() for word in words if word.lower() not in stop_words])\n",
    "\n",
    "# Prepare the data\n",
    "texts = [preprocess(words) for words, label in documents]\n",
    "labels = [label for _, label in documents]\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "# Convert text data into numerical features using TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=5000)  # Limit to top 5000 features\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# Train a classifier (e.g., Naive Bayes)\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = classifier.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate the model\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voici les étapes principales du processus **TF-IDF (Term Frequency-Inverse Document Frequency)** :\n",
    "\n",
    "1. **Calcul du Term Frequency (TF)** :\n",
    "   - Le **TF** mesure la fréquence d'apparition d'un mot dans un document par rapport à l'ensemble du document. \n",
    "   - Formellement : \n",
    "     \\[\n",
    "     TF(t, d) = \\frac{\\text{Nombre d'occurrences du terme } t \\text{ dans le document } d}{\\text{Nombre total de termes dans le document } d}\n",
    "     \\]\n",
    "\n",
    "2. **Calcul de l'Inverse Document Frequency (IDF)** :\n",
    "   - L'**IDF** mesure l'importance d'un terme dans l'ensemble des documents. Plus un mot apparaît dans peu de documents, plus il est important.\n",
    "   - Formellement :\n",
    "     \\[\n",
    "     IDF(t) = \\log\\left(\\frac{\\text{Nombre total de documents}}{\\text{Nombre de documents contenant le terme } t}\\right)\n",
    "     \\]\n",
    "\n",
    "3. **Calcul du TF-IDF** :\n",
    "   - Le **TF-IDF** est le produit du **TF** et du **IDF**, ce qui permet de mesurer l'importance d'un terme dans un document par rapport à l'ensemble du corpus.\n",
    "   - Formellement :\n",
    "     \\[\n",
    "     TFIDF(t, d) = TF(t, d) \\times IDF(t)\n",
    "     \\]\n",
    "\n",
    "---\n",
    "\n",
    "### **Résumé du processus** :\n",
    "1. Calculez le **TF** pour chaque mot dans chaque document.\n",
    "2. Calculez le **IDF** pour chaque mot dans l'ensemble des documents.\n",
    "3. Multipliez les valeurs **TF** et **IDF** pour chaque mot dans chaque document pour obtenir le **TF-IDF**.\n",
    "\n",
    "Le **TF-IDF** donne plus de poids aux mots fréquents dans un document, mais rares dans l'ensemble du corpus, ce qui permet de capturer les termes significatifs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition (NER) 🧐 Challenge:\n",
    "Implement a Named Entity Recognition algorithm that identifies and classifies named entities like names, locations, and organizations in a text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Similarity 📜 Challenge: \n",
    "Create a function that measures the similarity between two text documents using techniques like TF-IDF or cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TfidfVectorizer\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpairwise\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cosine_similarity\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "\n",
    "# Function to clean the text (optional step to remove special characters)\n",
    "def clean_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    return text\n",
    "\n",
    "# Function to compute similarity between two documents\n",
    "def compute_similarity(doc1, doc2):\n",
    "    # Clean the documents (optional)\n",
    "    doc1 = clean_text(doc1)\n",
    "    doc2 = clean_text(doc2)\n",
    "    \n",
    "    # Initialize TF-IDF Vectorizer\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    \n",
    "    # Fit and transform the documents\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform([doc1, doc2])\n",
    "    \n",
    "    # Compute cosine similarity between the two document vectors\n",
    "    cosine_sim = cosine_similarity(tfidf_matrix[0], tfidf_matrix[1])\n",
    "    \n",
    "    return cosine_sim[0][0]  # Cosine similarity score between 0 and 1\n",
    "\n",
    "# Example usage\n",
    "doc1 = \"I love programming in Python. It's a great language for data science.\"\n",
    "doc2 = \"Python is awesome for data science and machine learning tasks.\"\n",
    "\n",
    "similarity = compute_similarity(doc1, doc2)\n",
    "print(f\"Cosine Similarity: {similarity}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling 📌 Challenge: \n",
    "Use techniques like Latent Dirichlet Allocation (LDA) to perform topic modeling on a collection of documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le **Latent Dirichlet Allocation (LDA)** est un modèle statistique utilisé pour la **modélisation de sujets** dans des corpus de textes. Il permet de découvrir des structures latentes (sujets) dans un ensemble de documents sans supervision, en considérant que chaque document est une combinaison de plusieurs sujets. Voici un résumé des étapes et du concept d'**LDA** :\n",
    "\n",
    "---\n",
    "\n",
    "### **Concept de LDA**\n",
    "LDA est un modèle de **génération de texte** qui suppose que chaque document est un mélange de plusieurs sujets, et que chaque mot dans un document est associé à un sujet spécifique. L'objectif de LDA est de **découvrir** ces sujets latents en analysant la distribution des mots dans les documents.\n",
    "\n",
    "#### **Hypothèses principales** :\n",
    "1. Chaque document est constitué d’un **mélange** de sujets (thèmes).\n",
    "2. Chaque sujet est une distribution sur les mots dans le vocabulaire.\n",
    "3. LDA cherche à découvrir la répartition de ces sujets et mots à partir des données observées.\n",
    "\n",
    "---\n",
    "\n",
    "### **Étapes du processus LDA**\n",
    "\n",
    "1. **Initialisation** :\n",
    "   - Chaque mot d'un document est attribué aléatoirement à un sujet.\n",
    "   - On initialise les distributions de sujets pour chaque document et les distributions de mots pour chaque sujet.\n",
    "\n",
    "2. **Estimation des paramètres** :\n",
    "   - LDA utilise une **approche bayésienne** pour estimer les paramètres du modèle. L'idée est d'itérer sur le processus de **réaffectation** des mots à un sujet et de mettre à jour les distributions de manière à maximiser la vraisemblance des données.\n",
    "   \n",
    "   Les deux principales distributions sont :\n",
    "   - \\( \\theta_d \\) : La distribution des sujets pour le document \\( d \\).\n",
    "   - \\( \\phi_k \\) : La distribution des mots pour le sujet \\( k \\).\n",
    "\n",
    "3. **Mise à jour des affectations** :\n",
    "   - À chaque itération, chaque mot est réaffecté à un sujet en fonction de deux critères :\n",
    "     - La probabilité que ce mot vienne du sujet, basée sur la distribution de mots de chaque sujet.\n",
    "     - La probabilité que ce document soit associé à ce sujet, basée sur la distribution de sujets de ce document.\n",
    "\n",
    "4. **Répétition jusqu'à convergence** :\n",
    "   - Ce processus est répété plusieurs fois, ajustant continuellement les affectations de mots et les distributions des sujets jusqu'à ce que les distributions se stabilisent.\n",
    "\n",
    "5. **Finalisation du modèle** :\n",
    "   - Une fois l'algorithme convergé, chaque document peut être caractérisé par une distribution de sujets, et chaque sujet peut être décrit par une distribution de mots.\n",
    "\n",
    "---\n",
    "\n",
    "### **Résumé du processus LDA** :\n",
    "1. **Suppositions** : Chaque document est un mélange de sujets et chaque sujet est une distribution sur les mots.\n",
    "2. **Initialisation** : Affectation aléatoire des mots aux sujets.\n",
    "3. **Itération** :\n",
    "   - Mise à jour des affectations des mots aux sujets.\n",
    "   - Mise à jour des distributions des sujets et des mots.\n",
    "4. **Convergence** : L'algorithme converge vers un modèle stable.\n",
    "\n",
    "---\n",
    "\n",
    "### **Application de LDA** :\n",
    "- **Découverte de sujets** dans un corpus de texte.\n",
    "- **Réduction de dimension** pour des tâches comme la classification ou la recommandation.\n",
    "- **Compréhension du contenu** en extrayant des thèmes ou sujets sous-jacents.\n",
    "\n",
    "### **Exemple d'Application en Python (avec Gensim)** :\n",
    "```python\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Exemple de documents\n",
    "documents = [\"Machine learning is fascinating.\", \n",
    "             \"Natural language processing is a subfield of AI.\",\n",
    "             \"Deep learning is a subset of machine learning.\",\n",
    "             \"I love studying algorithms in computer science.\"]\n",
    "\n",
    "# Prétraitement des documents\n",
    "stop_words = set(stopwords.words('english'))\n",
    "processed_docs = [[word for word in word_tokenize(doc.lower()) if word.isalpha() and word not in stop_words] \n",
    "                  for doc in documents]\n",
    "\n",
    "# Créer un dictionnaire\n",
    "dictionary = corpora.Dictionary(processed_docs)\n",
    "\n",
    "# Créer le corpus\n",
    "corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "\n",
    "# Appliquer LDA\n",
    "lda_model = gensim.models.LdaMulticore(corpus, num_topics=2, id2word=dictionary, passes=10)\n",
    "\n",
    "# Afficher les topics\n",
    "topics = lda_model.print_topics(num_words=3)\n",
    "for topic in topics:\n",
    "    print(topic)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Avantages et Limites de LDA** :\n",
    "- **Avantages** :\n",
    "  - Identifie des sujets latents dans un corpus de textes sans supervision.\n",
    "  - Utile pour l'analyse de contenu à grande échelle (extraction de thèmes).\n",
    "  \n",
    "- **Limites** :\n",
    "  - Nécessite de spécifier le nombre de sujets à l'avance.\n",
    "  - La qualité des sujets dépend fortement de la prétraitement des données et de la sélection des hyperparamètres.\n",
    "\n",
    "LDA est un outil puissant pour explorer des corpus textuels et extraire des informations significatives sur les thèmes sous-jacents, mais il faut parfois expérimenter avec les hyperparamètres pour obtenir des résultats optimaux."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
