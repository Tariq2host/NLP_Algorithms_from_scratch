{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP basic challenges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Frequency Counter ðŸ“Š Challenge:\n",
    "Write a Python function that takes a text document as input and returns a dictionary containing the frequency of each word in the document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-basic approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def count_doc(doc_path):\n",
    "    vocab = {}\n",
    "    with open(doc_path, 'r', encoding='utf-8') as doc:\n",
    "        for line in doc:\n",
    "            # Remove punctuation and convert to lowercase\n",
    "            words = re.findall(r'\\b\\w+\\b', line.lower())\n",
    "            for word in words:\n",
    "                if word not in vocab:\n",
    "                    vocab[word] = 1\n",
    "                else:\n",
    "                    vocab[word] += 1\n",
    "    return vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 7,\n",
       " 'lost': 1,\n",
       " 'key': 4,\n",
       " 'once': 1,\n",
       " 'upon': 1,\n",
       " 'a': 2,\n",
       " 'time': 1,\n",
       " 'anna': 2,\n",
       " 'misplaced': 1,\n",
       " 'her': 4,\n",
       " 'she': 5,\n",
       " 'searched': 1,\n",
       " 'under': 1,\n",
       " 'couch': 1,\n",
       " 'inside': 1,\n",
       " 'bag': 1,\n",
       " 'and': 2,\n",
       " 'even': 1,\n",
       " 'in': 1,\n",
       " 'fridge': 1,\n",
       " 'but': 1,\n",
       " 'it': 1,\n",
       " 'was': 1,\n",
       " 'nowhere': 1,\n",
       " 'to': 3,\n",
       " 'be': 1,\n",
       " 'found': 2,\n",
       " 'frustrated': 1,\n",
       " 'sat': 1,\n",
       " 'down': 1,\n",
       " 'think': 1,\n",
       " 'suddenly': 1,\n",
       " 'remembered': 1,\n",
       " 'playing': 1,\n",
       " 'with': 1,\n",
       " 'cat': 2,\n",
       " 'whiskers': 3,\n",
       " 'earlier': 1,\n",
       " 'that': 1,\n",
       " 'morning': 1,\n",
       " 'could': 1,\n",
       " 'have': 1,\n",
       " 'taken': 1,\n",
       " 'followed': 1,\n",
       " 'living': 1,\n",
       " 'room': 1,\n",
       " 'where': 1,\n",
       " 'hidden': 1,\n",
       " 'beneath': 1,\n",
       " 'pile': 1,\n",
       " 'of': 1,\n",
       " 'cushions': 1,\n",
       " 'relieved': 1,\n",
       " 'laughed': 1,\n",
       " 'thanked': 1,\n",
       " 'for': 1,\n",
       " 'unexpected': 1,\n",
       " 'adventure': 1}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_doc(\"test.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2- map-reduce approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Concept de MapReduce :**\n",
    "MapReduce est un paradigme de programmation utilisÃ© pour traiter de grandes quantitÃ©s de donnÃ©es de maniÃ¨re distribuÃ©e. Il se divise en deux Ã©tapes principales :\n",
    "\n",
    "1. **Map** : \n",
    "   - Divise le travail en tÃ¢ches indÃ©pendantes.\n",
    "   - Transforme les donnÃ©es d'entrÃ©e en paires clÃ©-valeur `(clÃ©, valeur)`.\n",
    "   - Exemples : Compter les mots, gÃ©nÃ©rer `(mot, 1)` pour chaque mot trouvÃ©.\n",
    "\n",
    "2. **Reduce** :\n",
    "   - Regroupe les paires ayant la mÃªme clÃ©.\n",
    "   - Applique une opÃ©ration (comme une somme, une moyenne, etc.) pour rÃ©duire plusieurs valeurs associÃ©es Ã  une clÃ© en une seule.\n",
    "   - Exemples : Additionner les valeurs pour obtenir la frÃ©quence totale de chaque mot.\n",
    "\n",
    "---\n",
    "\n",
    "### **Ã‰tapes de MapReduce :**\n",
    "1. **Split** :\n",
    "   - Diviser les donnÃ©es d'entrÃ©e en morceaux (partitions).\n",
    "   - Exemple : Un fichier texte de 1 To divisÃ© en blocs de 64 Mo.\n",
    "\n",
    "2. **Map** :\n",
    "   - Traiter chaque morceau de maniÃ¨re indÃ©pendante pour gÃ©nÃ©rer des paires clÃ©-valeur.\n",
    "   - Exemple : Depuis un texte, produire `(mot, 1)` pour chaque mot.\n",
    "\n",
    "3. **Shuffle and Sort** :\n",
    "   - Regrouper et trier les donnÃ©es par clÃ© (toutes les occurrences de la mÃªme clÃ© ensemble).\n",
    "   - Exemple : Regrouper tous les `(mot, 1)` par mot.\n",
    "\n",
    "4. **Reduce** :\n",
    "   - Effectuer une opÃ©ration sur les groupes de clÃ©s triÃ©s.\n",
    "   - Exemple : Calculer la somme des valeurs pour chaque clÃ© `(mot, total)`.\n",
    "\n",
    "5. **Output** :\n",
    "   - Produire les rÃ©sultats finaux.\n",
    "   - Exemple : Une liste contenant chaque mot et sa frÃ©quence.\n",
    "\n",
    "---\n",
    "\n",
    "### **Illustration avec un exemple (comptage de mots) :**\n",
    "#### EntrÃ©e :\n",
    "```\n",
    "Texte : \"Bonjour monde, bonjour Python\"\n",
    "```\n",
    "\n",
    "#### Ã‰tapes :\n",
    "1. **Map** :\n",
    "   ```\n",
    "   Bonjour -> (bonjour, 1)\n",
    "   monde -> (monde, 1)\n",
    "   bonjour -> (bonjour, 1)\n",
    "   Python -> (python, 1)\n",
    "   ```\n",
    "\n",
    "2. **Shuffle and Sort** :\n",
    "   ```\n",
    "   (bonjour, [1, 1]), (monde, [1]), (python, [1])\n",
    "   ```\n",
    "\n",
    "3. **Reduce** :\n",
    "   ```\n",
    "   (bonjour, 2), (monde, 1), (python, 1)\n",
    "   ```\n",
    "\n",
    "#### RÃ©sultat final :\n",
    "```\n",
    "{'bonjour': 2, 'monde': 1, 'python': 1}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Avantages :**\n",
    "- **ScalabilitÃ©** : IdÃ©al pour traiter des donnÃ©es massives sur plusieurs machines.\n",
    "- **ParallÃ©lisme** : Les Ã©tapes Map et Reduce peuvent Ãªtre parallÃ©lisÃ©es.\n",
    "\n",
    "### **Limitation** :\n",
    "- Pas adaptÃ© Ã  tous les problÃ¨mes (par exemple, ceux nÃ©cessitant de fortes dÃ©pendances entre donnÃ©es)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def map_words(doc_path):\n",
    "    with open(doc_path, 'r', encoding='utf-8') as doc:\n",
    "        for line in doc:\n",
    "            words = re.findall(r'\\b\\w+\\b', line.lower())  # Clean and tokenize\n",
    "            for word in words:\n",
    "                yield (word, 1)  # Yield each word with a count of 1\n",
    "\n",
    "# Reduce function: aggregates counts for each word\n",
    "def reduce_word_counts(mapped_words):\n",
    "    word_counts = {}\n",
    "    for word, count in mapped_words:\n",
    "        if word not in word_counts:\n",
    "            word_counts[word] = 1  # Sum up counts for each word\n",
    "        else:\n",
    "            word_counts[word] += count\n",
    "    return word_counts\n",
    "\n",
    "\n",
    "# Combine Map and Reduce\n",
    "def map_reduce_word_count(doc_path):\n",
    "    mapped_words = map_words(doc_path)  # Step 1: Map\n",
    "    reduced_counts = reduce_word_counts(mapped_words)  # Step 2: Reduce\n",
    "    return reduced_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 7,\n",
       " 'lost': 1,\n",
       " 'key': 4,\n",
       " 'once': 1,\n",
       " 'upon': 1,\n",
       " 'a': 2,\n",
       " 'time': 1,\n",
       " 'anna': 2,\n",
       " 'misplaced': 1,\n",
       " 'her': 4,\n",
       " 'she': 5,\n",
       " 'searched': 1,\n",
       " 'under': 1,\n",
       " 'couch': 1,\n",
       " 'inside': 1,\n",
       " 'bag': 1,\n",
       " 'and': 2,\n",
       " 'even': 1,\n",
       " 'in': 1,\n",
       " 'fridge': 1,\n",
       " 'but': 1,\n",
       " 'it': 1,\n",
       " 'was': 1,\n",
       " 'nowhere': 1,\n",
       " 'to': 3,\n",
       " 'be': 1,\n",
       " 'found': 2,\n",
       " 'frustrated': 1,\n",
       " 'sat': 1,\n",
       " 'down': 1,\n",
       " 'think': 1,\n",
       " 'suddenly': 1,\n",
       " 'remembered': 1,\n",
       " 'playing': 1,\n",
       " 'with': 1,\n",
       " 'cat': 2,\n",
       " 'whiskers': 3,\n",
       " 'earlier': 1,\n",
       " 'that': 1,\n",
       " 'morning': 1,\n",
       " 'could': 1,\n",
       " 'have': 1,\n",
       " 'taken': 1,\n",
       " 'followed': 1,\n",
       " 'living': 1,\n",
       " 'room': 1,\n",
       " 'where': 1,\n",
       " 'hidden': 1,\n",
       " 'beneath': 1,\n",
       " 'pile': 1,\n",
       " 'of': 1,\n",
       " 'cushions': 1,\n",
       " 'relieved': 1,\n",
       " 'laughed': 1,\n",
       " 'thanked': 1,\n",
       " 'for': 1,\n",
       " 'unexpected': 1,\n",
       " 'adventure': 1}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_reduce_word_count(\"test.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Cleaning and Tokenization ðŸ§¹ Challenge:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function to clean and tokenize a given text, removing punctuation and converting words to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello,', 'world']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = \"hello, world\"\n",
    "test.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # lower the text\n",
    "    text = text.lower()\n",
    "    # remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    # tokenize\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['once',\n",
       " 'upon',\n",
       " 'a',\n",
       " 'time',\n",
       " 'anna',\n",
       " 'misplaced',\n",
       " 'her',\n",
       " 'key',\n",
       " 'she',\n",
       " 'searched',\n",
       " 'under',\n",
       " 'the',\n",
       " 'couch',\n",
       " 'inside',\n",
       " 'her',\n",
       " 'bag']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = \"Once upon a time, Anna misplaced her key. She searched under the couch, inside her bag,\"\n",
    "clean_text(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> re.sub(r'[^\\w\\s]', '', text) :\n",
    "\n",
    "Supprime tous les caractÃ¨res qui ne sont ni alphanumÃ©riques (\\w) ni des espaces (\\s).\n",
    "\n",
    "> re.sub(r'\\s+', ' ', text).strip() :\n",
    "\n",
    "Remplace plusieurs espaces par un seul, puis enlÃ¨ve les espaces superflus au dÃ©but ou Ã  la fin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopword Removal ðŸš« Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Develop a function that removes stopwords (commonly used words) from a given sentence or text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\tariq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# TÃ©lÃ©chargement des stop words (Ã  faire une seule fois)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    # DÃ©finir la liste des stop words en anglais\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    # Supprimer les stop words du texte\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Once upon time, Anna misplaced key. She searched couch, inside bag,'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = \"Once upon a time, Anna misplaced her key. She searched under the couch, inside her bag,\"\n",
    "remove_stop_words(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis ðŸŒŸ Challenge:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voici les Ã©tapes principales du processus **TF-IDF (Term Frequency-Inverse Document Frequency)** :\n",
    "\n",
    "1. **Calcul du Term Frequency (TF)** :\n",
    "   - Le **TF** mesure la frÃ©quence d'apparition d'un mot dans un document par rapport Ã  l'ensemble du document. \n",
    "   - Formellement : \n",
    "     \\[$\n",
    "     TF(t, d) = \\frac{\\text{Nombre d'occurrences du terme } t \\text{ dans le document } d}{\\text{Nombre total de termes dans le document } d}\n",
    "     $\\]\n",
    "\n",
    "2. **Calcul de l'Inverse Document Frequency (IDF)** :\n",
    "   - L'**IDF** mesure l'importance d'un terme dans l'ensemble des documents. Plus un mot apparaÃ®t dans peu de documents, plus il est important.\n",
    "   - Formellement :\n",
    "     \\[$\n",
    "     IDF(t) = \\log\\left(\\frac{\\text{Nombre total de documents}}{\\text{Nombre de documents contenant le terme } t}\\right)\n",
    "     $\\]\n",
    "\n",
    "3. **Calcul du TF-IDF** :\n",
    "   - Le **TF-IDF** est le produit du **TF** et du **IDF**, ce qui permet de mesurer l'importance d'un terme dans un document par rapport Ã  l'ensemble du corpus.\n",
    "   - Formellement :\n",
    "     \\[$\n",
    "     TFIDF(t, d) = TF(t, d) \\times IDF(t)\n",
    "     $\\]\n",
    "\n",
    "---\n",
    "\n",
    "### **RÃ©sumÃ© du processus** :\n",
    "1. Calculez le **TF** pour chaque mot dans chaque document.\n",
    "2. Calculez le **IDF** pour chaque mot dans l'ensemble des documents.\n",
    "3. Multipliez les valeurs **TF** et **IDF** pour chaque mot dans chaque document pour obtenir le **TF-IDF**.\n",
    "\n",
    "Le **TF-IDF** donne plus de poids aux mots frÃ©quents dans un document, mais rares dans l'ensemble du corpus, ce qui permet de capturer les termes significatifs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\tariq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\movie_reviews.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\tariq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.73      0.87      0.79       288\n",
      "         pos       0.86      0.70      0.77       312\n",
      "\n",
      "    accuracy                           0.78       600\n",
      "   macro avg       0.79      0.79      0.78       600\n",
      "weighted avg       0.80      0.78      0.78       600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Download required NLTK datasets\n",
    "nltk.download('movie_reviews')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load the dataset\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)]\n",
    "\n",
    "# Shuffle the dataset\n",
    "import random\n",
    "random.shuffle(documents)\n",
    "\n",
    "# Preprocess text (remove stop words and convert to lowercase)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess(words):\n",
    "    return ' '.join([word.lower() for word in words if word.lower() not in stop_words])\n",
    "\n",
    "# Prepare the data\n",
    "texts = [preprocess(words) for words, label in documents]\n",
    "labels = [label for _, label in documents]\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "# Convert text data into numerical features using TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=5000)  # Limit to top 5000 features\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# Train a classifier (e.g., Naive Bayes)\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = classifier.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate the model\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: This movie was absolutely fantastic! I loved it.\n",
      "Predicted Sentiment: pos\n",
      "------------------------------\n",
      "Review: The plot was boring and the acting was terrible.\n",
      "Predicted Sentiment: neg\n",
      "------------------------------\n",
      "Review: It was okay, not the best but not the worst either.\n",
      "Predicted Sentiment: neg\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test new reviews\n",
    "def test_classifier(new_reviews):\n",
    "    # Step 1: Preprocess the input reviews\n",
    "    preprocessed_reviews = [preprocess(review.split()) for review in new_reviews]\n",
    "    \n",
    "    # Step 2: Transform the reviews into TF-IDF features\n",
    "    tfidf_reviews = vectorizer.transform(preprocessed_reviews)\n",
    "    \n",
    "    # Step 3: Predict the sentiment\n",
    "    predictions = classifier.predict(tfidf_reviews)\n",
    "    \n",
    "    # Step 4: Output results\n",
    "    for review, sentiment in zip(new_reviews, predictions):\n",
    "        print(f\"Review: {review}\")\n",
    "        print(f\"Predicted Sentiment: {sentiment}\")\n",
    "        print(\"-\" * 30)\n",
    "\n",
    "# Example new reviews\n",
    "new_reviews = [\n",
    "    \"This movie was absolutely fantastic! I loved it.\",\n",
    "    \"The plot was boring and the acting was terrible.\",\n",
    "    \"It was okay, not the best but not the worst either.\"\n",
    "]\n",
    "\n",
    "# Test the classifier\n",
    "test_classifier(new_reviews)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ATTENTION: \n",
    ">> TF-IDF est une mÃ©thode efficace pour transformer un document en vecteur, mais ce n'est pas un modÃ¨le embedding car il ne capture pas les >> relations contextuelles ou sÃ©mantiques entre les mots comme le font Word2Vec, GloVe, ou BERT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition (NER) ðŸ§ Challenge:\n",
    "Implement a Named Entity Recognition algorithm that identifies and classifies named entities like names, locations, and organizations in a text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     -- ------------------------------------- 0.8/12.8 MB 4.2 MB/s eta 0:00:03\n",
      "     ----- ---------------------------------- 1.8/12.8 MB 5.0 MB/s eta 0:00:03\n",
      "     --------- ------------------------------ 3.1/12.8 MB 5.1 MB/s eta 0:00:02\n",
      "     ------------- -------------------------- 4.2/12.8 MB 5.2 MB/s eta 0:00:02\n",
      "     ---------------- ----------------------- 5.2/12.8 MB 5.1 MB/s eta 0:00:02\n",
      "     ------------------- -------------------- 6.3/12.8 MB 5.1 MB/s eta 0:00:02\n",
      "     ---------------------- ----------------- 7.3/12.8 MB 5.1 MB/s eta 0:00:02\n",
      "     --------------------------- ------------ 8.7/12.8 MB 5.2 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 10.0/12.8 MB 5.4 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 10.7/12.8 MB 5.3 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 11.5/12.8 MB 5.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.6/12.8 MB 5.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 12.8/12.8 MB 5.0 MB/s eta 0:00:00\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ip (c:\\Users\\tariq\\Desktop\\Quora_nlp\\.venv\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (c:\\Users\\tariq\\Desktop\\Quora_nlp\\.venv\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (c:\\Users\\tariq\\Desktop\\Quora_nlp\\.venv\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORG: ['Apple Inc.']\n",
      "GPE: ['Cupertino', 'California']\n",
      "PERSON: ['Steve Jobs', 'Steve Wozniak', 'Ronald Wayne']\n",
      "DATE: ['1976']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "def named_entity_recognition(text):\n",
    "    \"\"\"\n",
    "    Identifie et classe les entitÃ©s nommÃ©es dans un texte.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Le texte d'entrÃ©e pour l'analyse NER.\n",
    "        \n",
    "    Returns:\n",
    "        dict: Un dictionnaire avec des catÃ©gories d'entitÃ©s comme clÃ©s\n",
    "              et les entitÃ©s correspondantes comme valeurs (listes).\n",
    "    \"\"\"\n",
    "    # Charger un modÃ¨le prÃ©-entraÃ®nÃ© de spaCy\n",
    "    nlp = spacy.load(\"en_core_web_sm\")  # Petit modÃ¨le anglais\n",
    "\n",
    "    # Analyser le texte\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Initialiser un dictionnaire pour stocker les entitÃ©s\n",
    "    entities = {}\n",
    "\n",
    "    # Parcourir les entitÃ©s identifiÃ©es\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ not in entities:\n",
    "            entities[ent.label_] = []\n",
    "        entities[ent.label_].append(ent.text)\n",
    "\n",
    "    return entities\n",
    "\n",
    "# Exemple d'utilisation\n",
    "if __name__ == \"__main__\":\n",
    "    text = \"\"\"Apple Inc., based in Cupertino, California, was founded by Steve Jobs, \n",
    "    Steve Wozniak, and Ronald Wayne in 1976. It is one of the largest technology \n",
    "    companies in the world.\"\"\"\n",
    "    \n",
    "    result = named_entity_recognition(text)\n",
    "    for entity_type, entity_list in result.items():\n",
    "        print(f\"{entity_type}: {entity_list}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Similarity ðŸ“œ Challenge: \n",
    "Create a function that measures the similarity between two text documents using techniques like TF-IDF or cosine similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Proche de 1 : Les objets sont trÃ¨s similaires.\n",
    "> - Proche de 0 : Les objets ne sont pas similaires.\n",
    "> - Proche de -1 : Les objets sont opposÃ©s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: 0.25233420143369617\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "\n",
    "# Function to clean the text (optional step to remove special characters)\n",
    "def clean_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    return text\n",
    "\n",
    "# Function to compute similarity between two documents\n",
    "def compute_similarity(doc1, doc2):\n",
    "    # Clean the documents (optional)\n",
    "    doc1 = clean_text(doc1)\n",
    "    doc2 = clean_text(doc2)\n",
    "    \n",
    "    # Initialize TF-IDF Vectorizer\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    \n",
    "    # Fit and transform the documents\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform([doc1, doc2])\n",
    "    \n",
    "    # Compute cosine similarity between the two document vectors\n",
    "    cosine_sim = cosine_similarity(tfidf_matrix[0], tfidf_matrix[1])\n",
    "    \n",
    "    return cosine_sim[0][0]  # Cosine similarity score between 0 and 1\n",
    "\n",
    "# Example usage\n",
    "doc1 = \"I love programming in Python. It's a great language for data science.\"\n",
    "doc2 = \"Python is awesome for data science and machine learning tasks.\"\n",
    "\n",
    "similarity = compute_similarity(doc1, doc2)\n",
    "print(f\"Cosine Similarity: {similarity}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: 0.25233420143369617\n",
      "Jaccard Similarity: 0.2222222222222222\n",
      "Euclidean Distance: 1.222837518696825\n",
      "Manhattan Distance: 4.236033423568715\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.spatial.distance import euclidean, cityblock\n",
    "\n",
    "# Function to clean the text (optional step to remove special characters)\n",
    "def clean_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    return text\n",
    "\n",
    "# Function to compute Cosine Similarity\n",
    "def compute_cosine_similarity(doc1, doc2):\n",
    "    # Initialize TF-IDF Vectorizer\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    \n",
    "    # Fit and transform the documents\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform([doc1, doc2])\n",
    "    \n",
    "    # Compute cosine similarity between the two document vectors\n",
    "    cosine_sim = cosine_similarity(tfidf_matrix[0], tfidf_matrix[1])\n",
    "    \n",
    "    return cosine_sim[0][0]  # Cosine similarity score between 0 and 1\n",
    "\n",
    "# Function to compute Jaccard Similarity\n",
    "def compute_jaccard_similarity(doc1, doc2):\n",
    "    # Tokenize and get unique words\n",
    "    words_doc1 = set(clean_text(doc1).split())\n",
    "    words_doc2 = set(clean_text(doc2).split())\n",
    "    \n",
    "    # Compute Jaccard Similarity\n",
    "    intersection = len(words_doc1.intersection(words_doc2))\n",
    "    union = len(words_doc1.union(words_doc2))\n",
    "    \n",
    "    return intersection / union  # Jaccard similarity between 0 and 1\n",
    "\n",
    "# Function to compute Euclidean Distance\n",
    "def compute_euclidean_distance(doc1, doc2):\n",
    "    # Initialize TF-IDF Vectorizer\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    \n",
    "    # Fit and transform the documents\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform([doc1, doc2])\n",
    "    \n",
    "    # Compute Euclidean distance between the two document vectors (flatten to 1D)\n",
    "    euclidean_dist = euclidean(tfidf_matrix[0].toarray().flatten(), tfidf_matrix[1].toarray().flatten())\n",
    "    \n",
    "    return euclidean_dist\n",
    "\n",
    "\n",
    "# Function to compute Manhattan Distance\n",
    "def compute_manhattan_distance(doc1, doc2):\n",
    "    # Initialize TF-IDF Vectorizer\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    \n",
    "    # Fit and transform the documents\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform([doc1, doc2])\n",
    "    \n",
    "    # Compute Manhattan (L1) distance between the two document vectors\n",
    "    manhattan_dist = cityblock(tfidf_matrix[0].toarray().flatten(), tfidf_matrix[1].toarray().flatten())\n",
    "    \n",
    "    return manhattan_dist\n",
    "\n",
    "# Example usage\n",
    "doc1 = \"I love programming in Python. It's a great language for data science.\"\n",
    "doc2 = \"Python is awesome for data science and machine learning tasks.\"\n",
    "\n",
    "# Clean and compute different metrics\n",
    "cosine_sim = compute_cosine_similarity(doc1, doc2)\n",
    "jaccard_sim = compute_jaccard_similarity(doc1, doc2)\n",
    "euclidean_dist = compute_euclidean_distance(doc1, doc2)\n",
    "manhattan_dist = compute_manhattan_distance(doc1, doc2)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Cosine Similarity: {cosine_sim}\")\n",
    "print(f\"Jaccard Similarity: {jaccard_sim}\")\n",
    "print(f\"Euclidean Distance: {euclidean_dist}\")\n",
    "print(f\"Manhattan Distance: {manhattan_dist}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling ðŸ“Œ Challenge: \n",
    "Use techniques like Latent Dirichlet Allocation (LDA) to perform topic modeling on a collection of documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le **Latent Dirichlet Allocation (LDA)** est un modÃ¨le statistique utilisÃ© pour la **modÃ©lisation de sujets** dans des corpus de textes. Il permet de dÃ©couvrir des structures latentes (sujets) dans un ensemble de documents sans supervision, en considÃ©rant que chaque document est une combinaison de plusieurs sujets. Voici un rÃ©sumÃ© des Ã©tapes et du concept d'**LDA** :\n",
    "\n",
    "---\n",
    "\n",
    "### **Concept de LDA**\n",
    "LDA est un modÃ¨le de **gÃ©nÃ©ration de texte** qui suppose que chaque document est un mÃ©lange de plusieurs sujets, et que chaque mot dans un document est associÃ© Ã  un sujet spÃ©cifique. L'objectif de LDA est de **dÃ©couvrir** ces sujets latents en analysant la distribution des mots dans les documents.\n",
    "\n",
    "#### **HypothÃ¨ses principales** :\n",
    "1. Chaque document est constituÃ© dâ€™un **mÃ©lange** de sujets (thÃ¨mes).\n",
    "2. Chaque sujet est une distribution sur les mots dans le vocabulaire.\n",
    "3. LDA cherche Ã  dÃ©couvrir la rÃ©partition de ces sujets et mots Ã  partir des donnÃ©es observÃ©es.\n",
    "\n",
    "---\n",
    "\n",
    "### **Ã‰tapes du processus LDA**\n",
    "\n",
    "1. **Initialisation** :\n",
    "   - Chaque mot d'un document est attribuÃ© alÃ©atoirement Ã  un sujet.\n",
    "   - On initialise les distributions de sujets pour chaque document et les distributions de mots pour chaque sujet.\n",
    "\n",
    "2. **Estimation des paramÃ¨tres** :\n",
    "   - LDA utilise une **approche bayÃ©sienne** pour estimer les paramÃ¨tres du modÃ¨le. L'idÃ©e est d'itÃ©rer sur le processus de **rÃ©affectation** des mots Ã  un sujet et de mettre Ã  jour les distributions de maniÃ¨re Ã  maximiser la vraisemblance des donnÃ©es.\n",
    "   \n",
    "   Les deux principales distributions sont :\n",
    "   - \\( \\theta_d \\) : La distribution des sujets pour le document \\( d \\).\n",
    "   - \\( \\phi_k \\) : La distribution des mots pour le sujet \\( k \\).\n",
    "\n",
    "3. **Mise Ã  jour des affectations** :\n",
    "   - Ã€ chaque itÃ©ration, chaque mot est rÃ©affectÃ© Ã  un sujet en fonction de deux critÃ¨res :\n",
    "     - La probabilitÃ© que ce mot vienne du sujet, basÃ©e sur la distribution de mots de chaque sujet.\n",
    "     - La probabilitÃ© que ce document soit associÃ© Ã  ce sujet, basÃ©e sur la distribution de sujets de ce document.\n",
    "\n",
    "4. **RÃ©pÃ©tition jusqu'Ã  convergence** :\n",
    "   - Ce processus est rÃ©pÃ©tÃ© plusieurs fois, ajustant continuellement les affectations de mots et les distributions des sujets jusqu'Ã  ce que les distributions se stabilisent.\n",
    "\n",
    "5. **Finalisation du modÃ¨le** :\n",
    "   - Une fois l'algorithme convergÃ©, chaque document peut Ãªtre caractÃ©risÃ© par une distribution de sujets, et chaque sujet peut Ãªtre dÃ©crit par une distribution de mots.\n",
    "\n",
    "---\n",
    "\n",
    "### **RÃ©sumÃ© du processus LDA** :\n",
    "1. **Suppositions** : Chaque document est un mÃ©lange de sujets et chaque sujet est une distribution sur les mots.\n",
    "2. **Initialisation** : Affectation alÃ©atoire des mots aux sujets.\n",
    "3. **ItÃ©ration** :\n",
    "   - Mise Ã  jour des affectations des mots aux sujets.\n",
    "   - Mise Ã  jour des distributions des sujets et des mots.\n",
    "4. **Convergence** : L'algorithme converge vers un modÃ¨le stable.\n",
    "\n",
    "---\n",
    "\n",
    "### **Application de LDA** :\n",
    "- **DÃ©couverte de sujets** dans un corpus de texte.\n",
    "- **RÃ©duction de dimension** pour des tÃ¢ches comme la classification ou la recommandation.\n",
    "- **ComprÃ©hension du contenu** en extrayant des thÃ¨mes ou sujets sous-jacents.\n",
    "\n",
    "---\n",
    "\n",
    "### **Avantages et Limites de LDA** :\n",
    "- **Avantages** :\n",
    "  - Identifie des sujets latents dans un corpus de textes sans supervision.\n",
    "  - Utile pour l'analyse de contenu Ã  grande Ã©chelle (extraction de thÃ¨mes).\n",
    "  \n",
    "- **Limites** :\n",
    "  - NÃ©cessite de spÃ©cifier le nombre de sujets Ã  l'avance.\n",
    "  - La qualitÃ© des sujets dÃ©pend fortement de la prÃ©traitement des donnÃ©es et de la sÃ©lection des hyperparamÃ¨tres.\n",
    "\n",
    "LDA est un outil puissant pour explorer des corpus textuels et extraire des informations significatives sur les thÃ¨mes sous-jacents, mais il faut parfois expÃ©rimenter avec les hyperparamÃ¨tres pour obtenir des rÃ©sultats optimaux."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\tariq\\desktop\\quora_nlp\\.venv\\lib\\site-packages (3.9.1)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: click in c:\\users\\tariq\\desktop\\quora_nlp\\.venv\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\tariq\\desktop\\quora_nlp\\.venv\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\tariq\\desktop\\quora_nlp\\.venv\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\tariq\\desktop\\quora_nlp\\.venv\\lib\\site-packages (from nltk) (4.67.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\tariq\\desktop\\quora_nlp\\.venv\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ip (c:\\Users\\tariq\\Desktop\\Quora_nlp\\.venv\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (c:\\Users\\tariq\\Desktop\\Quora_nlp\\.venv\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (c:\\Users\\tariq\\Desktop\\Quora_nlp\\.venv\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\tariq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\tariq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\tariq/nltk_data'\n    - 'c:\\\\Users\\\\tariq\\\\Desktop\\\\Quora_nlp\\\\.venv\\\\nltk_data'\n    - 'c:\\\\Users\\\\tariq\\\\Desktop\\\\Quora_nlp\\\\.venv\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\tariq\\\\Desktop\\\\Quora_nlp\\\\.venv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\tariq\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# PrÃ©traitement des documents\u001b[39;00m\n\u001b[0;32m     18\u001b[0m stop_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(stopwords\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m---> 19\u001b[0m processed_docs \u001b[38;5;241m=\u001b[39m [[word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m word\u001b[38;5;241m.\u001b[39misalpha() \u001b[38;5;129;01mand\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words] \n\u001b[0;32m     20\u001b[0m                   \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# CrÃ©er un dictionnaire\u001b[39;00m\n\u001b[0;32m     23\u001b[0m dictionary \u001b[38;5;241m=\u001b[39m corpora\u001b[38;5;241m.\u001b[39mDictionary(processed_docs)\n",
      "File \u001b[1;32mc:\\Users\\tariq\\Desktop\\Quora_nlp\\.venv\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:142\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 142\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    144\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[0;32m    145\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\tariq\\Desktop\\Quora_nlp\\.venv\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32mc:\\Users\\tariq\\Desktop\\Quora_nlp\\.venv\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[1;34m(language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_punkt_tokenizer\u001b[39m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;124;03m    a lru cache for performance.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m    :type language: str\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tariq\\Desktop\\Quora_nlp\\.venv\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m-> 1744\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tariq\\Desktop\\Quora_nlp\\.venv\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[1;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[0;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
      "File \u001b[1;32mc:\\Users\\tariq\\Desktop\\Quora_nlp\\.venv\\Lib\\site-packages\\nltk\\data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\tariq/nltk_data'\n    - 'c:\\\\Users\\\\tariq\\\\Desktop\\\\Quora_nlp\\\\.venv\\\\nltk_data'\n    - 'c:\\\\Users\\\\tariq\\\\Desktop\\\\Quora_nlp\\\\.venv\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\tariq\\\\Desktop\\\\Quora_nlp\\\\.venv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\tariq\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Exemple de documents\n",
    "documents = [\"Machine learning is fascinating.\", \n",
    "             \"Natural language processing is a subfield of AI.\",\n",
    "             \"Deep learning is a subset of machine learning.\",\n",
    "             \"I love studying algorithms in computer science.\"]\n",
    "\n",
    "# PrÃ©traitement des documents\n",
    "stop_words = set(stopwords.words('english'))\n",
    "processed_docs = [[word for word in word_tokenize(doc.lower()) if word.isalpha() and word not in stop_words] \n",
    "                  for doc in documents]\n",
    "\n",
    "# CrÃ©er un dictionnaire\n",
    "dictionary = corpora.Dictionary(processed_docs)\n",
    "\n",
    "# CrÃ©er le corpus\n",
    "corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "\n",
    "# Appliquer LDA\n",
    "lda_model = gensim.models.LdaMulticore(corpus, num_topics=2, id2word=dictionary, passes=10)\n",
    "\n",
    "# Afficher les topics\n",
    "topics = lda_model.print_topics(num_words=3)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimilaritÃ© entre les deux textes : 0.73\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "# Ensure NLTK resources are downloaded\n",
    "try:\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "except:\n",
    "    print(\"NLTK resources download might have failed. Proceeding with alternative tokenization.\")\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Load a pre-trained word embedding model\n",
    "try:\n",
    "    word2vec = api.load('glove-wiki-gigaword-100')\n",
    "except Exception as e:\n",
    "    print(\"Failed to load pre-trained model. Using a simple alternative.\")\n",
    "    word2vec = None\n",
    "\n",
    "# Alternative tokenization function\n",
    "def simple_tokenize(text):\n",
    "    # Remove punctuation and convert to lowercase\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Split on whitespace\n",
    "    tokens = text.split()\n",
    "    \n",
    "    # Remove stopwords\n",
    "    try:\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [word for word in tokens if word not in stop_words]\n",
    "    except:\n",
    "        # Fallback if stopwords can't be loaded\n",
    "        tokens = [word for word in tokens if len(word) > 1]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Obtenir le vecteur moyen pour un texte\n",
    "def text_to_vector(text, word2vec_model):\n",
    "    if word2vec_model is None:\n",
    "        # Simple fallback if no pre-trained model is available\n",
    "        tokens = simple_tokenize(text)\n",
    "        return np.array([len(token) for token in tokens])\n",
    "    \n",
    "    tokens = simple_tokenize(text)\n",
    "    word_vectors = [word2vec_model[word] for word in tokens if word in word2vec_model]\n",
    "    \n",
    "    if len(word_vectors) == 0:\n",
    "        return np.zeros(word2vec_model.vector_size)  # Vecteur nul si aucun mot n'est dans le vocabulaire\n",
    "    \n",
    "    return np.mean(word_vectors, axis=0)\n",
    "\n",
    "# Calculer la similaritÃ© entre deux textes\n",
    "def compute_similarity(text1, text2, word2vec_model):\n",
    "    vec1 = text_to_vector(text1, word2vec_model)\n",
    "    vec2 = text_to_vector(text2, word2vec_model)\n",
    "    \n",
    "    # Handle edge cases\n",
    "    if np.all(vec1 == 0) or np.all(vec2 == 0):\n",
    "        return 0.0\n",
    "    \n",
    "    similarity = cosine_similarity([vec1], [vec2])[0][0]  # SimilaritÃ© cosinus\n",
    "    return similarity\n",
    "\n",
    "# Exemple\n",
    "text1 = \"The cat sat on the mat.\"\n",
    "text2 = \"The cat lay on the rug.\"\n",
    "\n",
    "similarity = compute_similarity(text1, text2, word2vec)\n",
    "print(f\"SimilaritÃ© entre les deux textes : {similarity:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimilaritÃ© entre les deux textes : 0.47\n"
     ]
    }
   ],
   "source": [
    "# Exemple\n",
    "text1 = \"The cat sat on the mat.\"\n",
    "text2 = \"The human is crying\"\n",
    "\n",
    "similarity = compute_similarity(text1, text2, word2vec)\n",
    "print(f\"SimilaritÃ© entre les deux textes : {similarity:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. **Text Generation ðŸ–‹ï¸**\n",
    "   - **Challenge**: Build a text generation model (e.g., RNNs then LSTM) that generates coherent sentences based on a given input.\n",
    "   - **Solution**: This solution builds a model capable of generating new text based on patterns learned from a corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tariq\\Desktop\\Quora_nlp\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - accuracy: 0.0164 - loss: 3.9515\n",
      "Epoch 2/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.0492 - loss: 3.9442\n",
      "Epoch 3/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.1148 - loss: 3.9368\n",
      "Epoch 4/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.1311 - loss: 3.9289\n",
      "Epoch 5/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.1311 - loss: 3.9202\n",
      "Epoch 6/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.1639 - loss: 3.9102\n",
      "Epoch 7/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.1475 - loss: 3.8983\n",
      "Epoch 8/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.0984 - loss: 3.8837\n",
      "Epoch 9/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.0984 - loss: 3.8655\n",
      "Epoch 10/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.0984 - loss: 3.8427\n",
      "Epoch 11/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.0984 - loss: 3.8144\n",
      "Epoch 12/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.0984 - loss: 3.7820\n",
      "Epoch 13/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.0984 - loss: 3.7509\n",
      "Epoch 14/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.1311 - loss: 3.7298\n",
      "Epoch 15/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.0984 - loss: 3.7157\n",
      "Epoch 16/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.0984 - loss: 3.6950\n",
      "Epoch 17/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.1148 - loss: 3.6663\n",
      "Epoch 18/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.1148 - loss: 3.6368\n",
      "Epoch 19/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.0984 - loss: 3.6116\n",
      "Epoch 20/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.0656 - loss: 3.5893\n",
      "The future of AI the the the the the the the plain plain plain dog dog dog dog dog dog the the the the plain plain plain plain plain plain dog dog plain the the the the plain plain plain plain plain plain dog dog plain the the the the plain plain plain plain\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "# Step 1: Prepare the Data\n",
    "\n",
    "# Load your text corpus\n",
    "with open(\"test_corpus.txt\", \"r\") as file:\n",
    "    text = file.read().lower()\n",
    "\n",
    "# Tokenize the text into words\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([text])\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Create sequences of words for training the model\n",
    "input_sequences = []\n",
    "for line in text.split(\"\\n\"):\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "\n",
    "# Pad the sequences to ensure equal input lengths\n",
    "max_sequence_length = max([len(seq) for seq in input_sequences])\n",
    "input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_length, padding='pre')\n",
    "\n",
    "X, y = input_sequences[:, :-1], input_sequences[:, -1]\n",
    "y = np.expand_dims(y, axis=-1)\n",
    "\n",
    "# Step 2: Build the Model\n",
    "\n",
    "# Define the model architecture\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 100, input_length=max_sequence_length-1))\n",
    "model.add(LSTM(150, return_sequences=False))\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Step 3: Train the Model\n",
    "model.fit(X, y, epochs=20, batch_size=64)\n",
    "\n",
    "# Step 4: Generate Text\n",
    "\n",
    "def generate_text(seed_text, model, tokenizer, max_sequence_length, num_words=50):\n",
    "    for _ in range(num_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_length-1, padding='pre')\n",
    "        \n",
    "        predicted_probs = model.predict(token_list, verbose=0)\n",
    "        predicted_word_index = np.argmax(predicted_probs, axis=-1)[0]\n",
    "        \n",
    "        predicted_word = tokenizer.index_word[predicted_word_index]\n",
    "        seed_text += \" \" + predicted_word\n",
    "        \n",
    "    return seed_text\n",
    "\n",
    "# Generate new text based on a seed input\n",
    "seed_text = \"The future of AI\"\n",
    "generated_text = generate_text(seed_text, model, tokenizer, max_sequence_length)\n",
    "print(generated_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
