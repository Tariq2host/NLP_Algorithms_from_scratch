{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP basic challenges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Frequency Counter üìä Challenge:\n",
    "Write a Python function that takes a text document as input and returns a dictionary containing the frequency of each word in the document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-basic approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def count_doc(doc_path):\n",
    "    vocab = {}\n",
    "    with open(doc_path, 'r', encoding='utf-8') as doc:\n",
    "        for line in doc:\n",
    "            # Remove punctuation and convert to lowercase\n",
    "            words = re.findall(r'\\b\\w+\\b', line.lower())\n",
    "            for word in words:\n",
    "                if word not in vocab:\n",
    "                    vocab[word] = 1\n",
    "                else:\n",
    "                    vocab[word] += 1\n",
    "    return vocab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2- map-reduce approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Concept de MapReduce :**\n",
    "MapReduce est un paradigme de programmation utilis√© pour traiter de grandes quantit√©s de donn√©es de mani√®re distribu√©e. Il se divise en deux √©tapes principales :\n",
    "\n",
    "1. **Map** : \n",
    "   - Divise le travail en t√¢ches ind√©pendantes.\n",
    "   - Transforme les donn√©es d'entr√©e en paires cl√©-valeur `(cl√©, valeur)`.\n",
    "   - Exemples : Compter les mots, g√©n√©rer `(mot, 1)` pour chaque mot trouv√©.\n",
    "\n",
    "2. **Reduce** :\n",
    "   - Regroupe les paires ayant la m√™me cl√©.\n",
    "   - Applique une op√©ration (comme une somme, une moyenne, etc.) pour r√©duire plusieurs valeurs associ√©es √† une cl√© en une seule.\n",
    "   - Exemples : Additionner les valeurs pour obtenir la fr√©quence totale de chaque mot.\n",
    "\n",
    "---\n",
    "\n",
    "### **√âtapes de MapReduce :**\n",
    "1. **Split** :\n",
    "   - Diviser les donn√©es d'entr√©e en morceaux (partitions).\n",
    "   - Exemple : Un fichier texte de 1 To divis√© en blocs de 64 Mo.\n",
    "\n",
    "2. **Map** :\n",
    "   - Traiter chaque morceau de mani√®re ind√©pendante pour g√©n√©rer des paires cl√©-valeur.\n",
    "   - Exemple : Depuis un texte, produire `(mot, 1)` pour chaque mot.\n",
    "\n",
    "3. **Shuffle and Sort** :\n",
    "   - Regrouper et trier les donn√©es par cl√© (toutes les occurrences de la m√™me cl√© ensemble).\n",
    "   - Exemple : Regrouper tous les `(mot, 1)` par mot.\n",
    "\n",
    "4. **Reduce** :\n",
    "   - Effectuer une op√©ration sur les groupes de cl√©s tri√©s.\n",
    "   - Exemple : Calculer la somme des valeurs pour chaque cl√© `(mot, total)`.\n",
    "\n",
    "5. **Output** :\n",
    "   - Produire les r√©sultats finaux.\n",
    "   - Exemple : Une liste contenant chaque mot et sa fr√©quence.\n",
    "\n",
    "---\n",
    "\n",
    "### **Illustration avec un exemple (comptage de mots) :**\n",
    "#### Entr√©e :\n",
    "```\n",
    "Texte : \"Bonjour monde, bonjour Python\"\n",
    "```\n",
    "\n",
    "#### √âtapes :\n",
    "1. **Map** :\n",
    "   ```\n",
    "   Bonjour -> (bonjour, 1)\n",
    "   monde -> (monde, 1)\n",
    "   bonjour -> (bonjour, 1)\n",
    "   Python -> (python, 1)\n",
    "   ```\n",
    "\n",
    "2. **Shuffle and Sort** :\n",
    "   ```\n",
    "   (bonjour, [1, 1]), (monde, [1]), (python, [1])\n",
    "   ```\n",
    "\n",
    "3. **Reduce** :\n",
    "   ```\n",
    "   (bonjour, 2), (monde, 1), (python, 1)\n",
    "   ```\n",
    "\n",
    "#### R√©sultat final :\n",
    "```\n",
    "{'bonjour': 2, 'monde': 1, 'python': 1}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Avantages :**\n",
    "- **Scalabilit√©** : Id√©al pour traiter des donn√©es massives sur plusieurs machines.\n",
    "- **Parall√©lisme** : Les √©tapes Map et Reduce peuvent √™tre parall√©lis√©es.\n",
    "\n",
    "### **Limitation** :\n",
    "- Pas adapt√© √† tous les probl√®mes (par exemple, ceux n√©cessitant de fortes d√©pendances entre donn√©es)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def map_words(doc_path):\n",
    "    with open(doc_path, 'r', encoding='utf-8') as doc:\n",
    "        for line in doc:\n",
    "            words = re.findall(r'\\b\\w+\\b', line.lower())  # Clean and tokenize\n",
    "            for word in words:\n",
    "                yield (word, 1)  # Yield each word with a count of 1\n",
    "\n",
    "# Reduce function: aggregates counts for each word\n",
    "def reduce_word_counts(mapped_words):\n",
    "    word_counts = {}\n",
    "    for word, count in mapped_words:\n",
    "        if word not in word_counts:\n",
    "            word_counts[word] += 1  # Sum up counts for each word\n",
    "        else:\n",
    "            word_counts[word] += count\n",
    "    return word_counts\n",
    "\n",
    "\n",
    "# Combine Map and Reduce\n",
    "def map_reduce_word_count(doc_path):\n",
    "    mapped_words = map_words(doc_path)  # Step 1: Map\n",
    "    reduced_counts = reduce_word_counts(mapped_words)  # Step 2: Reduce\n",
    "    return reduced_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Cleaning and Tokenization üßπ Challenge:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function to clean and tokenize a given text, removing punctuation and converting words to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello,', 'world']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = \"hello, world\"\n",
    "test.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # lower the text\n",
    "    text = text.lower()\n",
    "    # remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    # tokenize\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> re.sub(r'[^\\w\\s]', '', text) :\n",
    "\n",
    "Supprime tous les caract√®res qui ne sont ni alphanum√©riques (\\w) ni des espaces (\\s).\n",
    "\n",
    "> re.sub(r'\\s+', ' ', text).strip() :\n",
    "\n",
    "Remplace plusieurs espaces par un seul, puis enl√®ve les espaces superflus au d√©but ou √† la fin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopword Removal üö´ Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Develop a function that removes stopwords (commonly used words) from a given sentence or text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m stopwords\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# T√©l√©chargement des stop words (√† faire une seule fois)\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# T√©l√©chargement des stop words (√† faire une seule fois)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    # D√©finir la liste des stop words en anglais\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    # Supprimer les stop words du texte\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis üåü Challenge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Download required NLTK datasets\n",
    "nltk.download('movie_reviews')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load the dataset\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)]\n",
    "\n",
    "# Shuffle the dataset\n",
    "import random\n",
    "random.shuffle(documents)\n",
    "\n",
    "# Preprocess text (remove stop words and convert to lowercase)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess(words):\n",
    "    return ' '.join([word.lower() for word in words if word.lower() not in stop_words])\n",
    "\n",
    "# Prepare the data\n",
    "texts = [preprocess(words) for words, label in documents]\n",
    "labels = [label for _, label in documents]\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "# Convert text data into numerical features using TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=5000)  # Limit to top 5000 features\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# Train a classifier (e.g., Naive Bayes)\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = classifier.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate the model\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voici les √©tapes principales du processus **TF-IDF (Term Frequency-Inverse Document Frequency)** :\n",
    "\n",
    "1. **Calcul du Term Frequency (TF)** :\n",
    "   - Le **TF** mesure la fr√©quence d'apparition d'un mot dans un document par rapport √† l'ensemble du document. \n",
    "   - Formellement : \n",
    "     \\[\n",
    "     TF(t, d) = \\frac{\\text{Nombre d'occurrences du terme } t \\text{ dans le document } d}{\\text{Nombre total de termes dans le document } d}\n",
    "     \\]\n",
    "\n",
    "2. **Calcul de l'Inverse Document Frequency (IDF)** :\n",
    "   - L'**IDF** mesure l'importance d'un terme dans l'ensemble des documents. Plus un mot appara√Æt dans peu de documents, plus il est important.\n",
    "   - Formellement :\n",
    "     \\[\n",
    "     IDF(t) = \\log\\left(\\frac{\\text{Nombre total de documents}}{\\text{Nombre de documents contenant le terme } t}\\right)\n",
    "     \\]\n",
    "\n",
    "3. **Calcul du TF-IDF** :\n",
    "   - Le **TF-IDF** est le produit du **TF** et du **IDF**, ce qui permet de mesurer l'importance d'un terme dans un document par rapport √† l'ensemble du corpus.\n",
    "   - Formellement :\n",
    "     \\[\n",
    "     TFIDF(t, d) = TF(t, d) \\times IDF(t)\n",
    "     \\]\n",
    "\n",
    "---\n",
    "\n",
    "### **R√©sum√© du processus** :\n",
    "1. Calculez le **TF** pour chaque mot dans chaque document.\n",
    "2. Calculez le **IDF** pour chaque mot dans l'ensemble des documents.\n",
    "3. Multipliez les valeurs **TF** et **IDF** pour chaque mot dans chaque document pour obtenir le **TF-IDF**.\n",
    "\n",
    "Le **TF-IDF** donne plus de poids aux mots fr√©quents dans un document, mais rares dans l'ensemble du corpus, ce qui permet de capturer les termes significatifs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition (NER) üßê Challenge:\n",
    "Implement a Named Entity Recognition algorithm that identifies and classifies named entities like names, locations, and organizations in a text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Similarity üìú Challenge: \n",
    "Create a function that measures the similarity between two text documents using techniques like TF-IDF or cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TfidfVectorizer\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpairwise\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cosine_similarity\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "\n",
    "# Function to clean the text (optional step to remove special characters)\n",
    "def clean_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    return text\n",
    "\n",
    "# Function to compute similarity between two documents\n",
    "def compute_similarity(doc1, doc2):\n",
    "    # Clean the documents (optional)\n",
    "    doc1 = clean_text(doc1)\n",
    "    doc2 = clean_text(doc2)\n",
    "    \n",
    "    # Initialize TF-IDF Vectorizer\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    \n",
    "    # Fit and transform the documents\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform([doc1, doc2])\n",
    "    \n",
    "    # Compute cosine similarity between the two document vectors\n",
    "    cosine_sim = cosine_similarity(tfidf_matrix[0], tfidf_matrix[1])\n",
    "    \n",
    "    return cosine_sim[0][0]  # Cosine similarity score between 0 and 1\n",
    "\n",
    "# Example usage\n",
    "doc1 = \"I love programming in Python. It's a great language for data science.\"\n",
    "doc2 = \"Python is awesome for data science and machine learning tasks.\"\n",
    "\n",
    "similarity = compute_similarity(doc1, doc2)\n",
    "print(f\"Cosine Similarity: {similarity}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling üìå Challenge: \n",
    "Use techniques like Latent Dirichlet Allocation (LDA) to perform topic modeling on a collection of documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le **Latent Dirichlet Allocation (LDA)** est un mod√®le statistique utilis√© pour la **mod√©lisation de sujets** dans des corpus de textes. Il permet de d√©couvrir des structures latentes (sujets) dans un ensemble de documents sans supervision, en consid√©rant que chaque document est une combinaison de plusieurs sujets. Voici un r√©sum√© des √©tapes et du concept d'**LDA** :\n",
    "\n",
    "---\n",
    "\n",
    "### **Concept de LDA**\n",
    "LDA est un mod√®le de **g√©n√©ration de texte** qui suppose que chaque document est un m√©lange de plusieurs sujets, et que chaque mot dans un document est associ√© √† un sujet sp√©cifique. L'objectif de LDA est de **d√©couvrir** ces sujets latents en analysant la distribution des mots dans les documents.\n",
    "\n",
    "#### **Hypoth√®ses principales** :\n",
    "1. Chaque document est constitu√© d‚Äôun **m√©lange** de sujets (th√®mes).\n",
    "2. Chaque sujet est une distribution sur les mots dans le vocabulaire.\n",
    "3. LDA cherche √† d√©couvrir la r√©partition de ces sujets et mots √† partir des donn√©es observ√©es.\n",
    "\n",
    "---\n",
    "\n",
    "### **√âtapes du processus LDA**\n",
    "\n",
    "1. **Initialisation** :\n",
    "   - Chaque mot d'un document est attribu√© al√©atoirement √† un sujet.\n",
    "   - On initialise les distributions de sujets pour chaque document et les distributions de mots pour chaque sujet.\n",
    "\n",
    "2. **Estimation des param√®tres** :\n",
    "   - LDA utilise une **approche bay√©sienne** pour estimer les param√®tres du mod√®le. L'id√©e est d'it√©rer sur le processus de **r√©affectation** des mots √† un sujet et de mettre √† jour les distributions de mani√®re √† maximiser la vraisemblance des donn√©es.\n",
    "   \n",
    "   Les deux principales distributions sont :\n",
    "   - \\( \\theta_d \\) : La distribution des sujets pour le document \\( d \\).\n",
    "   - \\( \\phi_k \\) : La distribution des mots pour le sujet \\( k \\).\n",
    "\n",
    "3. **Mise √† jour des affectations** :\n",
    "   - √Ä chaque it√©ration, chaque mot est r√©affect√© √† un sujet en fonction de deux crit√®res :\n",
    "     - La probabilit√© que ce mot vienne du sujet, bas√©e sur la distribution de mots de chaque sujet.\n",
    "     - La probabilit√© que ce document soit associ√© √† ce sujet, bas√©e sur la distribution de sujets de ce document.\n",
    "\n",
    "4. **R√©p√©tition jusqu'√† convergence** :\n",
    "   - Ce processus est r√©p√©t√© plusieurs fois, ajustant continuellement les affectations de mots et les distributions des sujets jusqu'√† ce que les distributions se stabilisent.\n",
    "\n",
    "5. **Finalisation du mod√®le** :\n",
    "   - Une fois l'algorithme converg√©, chaque document peut √™tre caract√©ris√© par une distribution de sujets, et chaque sujet peut √™tre d√©crit par une distribution de mots.\n",
    "\n",
    "---\n",
    "\n",
    "### **R√©sum√© du processus LDA** :\n",
    "1. **Suppositions** : Chaque document est un m√©lange de sujets et chaque sujet est une distribution sur les mots.\n",
    "2. **Initialisation** : Affectation al√©atoire des mots aux sujets.\n",
    "3. **It√©ration** :\n",
    "   - Mise √† jour des affectations des mots aux sujets.\n",
    "   - Mise √† jour des distributions des sujets et des mots.\n",
    "4. **Convergence** : L'algorithme converge vers un mod√®le stable.\n",
    "\n",
    "---\n",
    "\n",
    "### **Application de LDA** :\n",
    "- **D√©couverte de sujets** dans un corpus de texte.\n",
    "- **R√©duction de dimension** pour des t√¢ches comme la classification ou la recommandation.\n",
    "- **Compr√©hension du contenu** en extrayant des th√®mes ou sujets sous-jacents.\n",
    "\n",
    "### **Exemple d'Application en Python (avec Gensim)** :\n",
    "```python\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Exemple de documents\n",
    "documents = [\"Machine learning is fascinating.\", \n",
    "             \"Natural language processing is a subfield of AI.\",\n",
    "             \"Deep learning is a subset of machine learning.\",\n",
    "             \"I love studying algorithms in computer science.\"]\n",
    "\n",
    "# Pr√©traitement des documents\n",
    "stop_words = set(stopwords.words('english'))\n",
    "processed_docs = [[word for word in word_tokenize(doc.lower()) if word.isalpha() and word not in stop_words] \n",
    "                  for doc in documents]\n",
    "\n",
    "# Cr√©er un dictionnaire\n",
    "dictionary = corpora.Dictionary(processed_docs)\n",
    "\n",
    "# Cr√©er le corpus\n",
    "corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "\n",
    "# Appliquer LDA\n",
    "lda_model = gensim.models.LdaMulticore(corpus, num_topics=2, id2word=dictionary, passes=10)\n",
    "\n",
    "# Afficher les topics\n",
    "topics = lda_model.print_topics(num_words=3)\n",
    "for topic in topics:\n",
    "    print(topic)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Avantages et Limites de LDA** :\n",
    "- **Avantages** :\n",
    "  - Identifie des sujets latents dans un corpus de textes sans supervision.\n",
    "  - Utile pour l'analyse de contenu √† grande √©chelle (extraction de th√®mes).\n",
    "  \n",
    "- **Limites** :\n",
    "  - N√©cessite de sp√©cifier le nombre de sujets √† l'avance.\n",
    "  - La qualit√© des sujets d√©pend fortement de la pr√©traitement des donn√©es et de la s√©lection des hyperparam√®tres.\n",
    "\n",
    "LDA est un outil puissant pour explorer des corpus textuels et extraire des informations significatives sur les th√®mes sous-jacents, mais il faut parfois exp√©rimenter avec les hyperparam√®tres pour obtenir des r√©sultats optimaux."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
