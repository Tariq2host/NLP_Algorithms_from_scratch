{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec from scratch\n",
    "\n",
    "## 16. Explain how Word2Vec learns? What is the loss function? What is maximized?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## build word2vec from scratch\n",
    "\n",
    "Word2Vec est un modèle de type embeddings qui représente les mots sous forme de vecteurs denses dans un espace de dimension réduite, de manière à capturer le contexte d'un mot dans une phrase.\n",
    "\n",
    "il apprend de deux manière\n",
    "- Skip-Gram : Pour chaque mot cible, prédire les mots de contexte autour de lui.\n",
    "- CBOW (Continuous Bag of Words) : Pour chaque ensemble de mots de contexte, prédire le mot cible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Utilisez Skip-gram si vous avez un petit corpus ou des mots rares à traiter.\n",
    "- Utilisez CBOW si vous travaillez avec un grand corpus et un vocabulaire plus fréquent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2vec from scratch with Skip-Gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "<img src= \"img/word2vec.png\"/>\n",
    "<div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['le', 'chat', 'mange', 'une', 'pomme'], ['le', 'chien', 'dort', 'sous', \"l'arbre\"], ['le', 'chat', 'dort', 'aussi', 'sous', \"l'arbre\"]]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Exemple de corpus\n",
    "corpus = [\n",
    "    \"le chat mange une pomme\",\n",
    "    \"le chien dort sous l'arbre\",\n",
    "    \"le chat dort aussi sous l'arbre\"\n",
    "]\n",
    "\n",
    "# Tokenisation du corpus\n",
    "tokens = [sentence.split() for sentence in corpus]\n",
    "print(tokens)\n",
    "\n",
    "# Création d'un dictionnaire de vocabulaire\n",
    "vocab = set(word for sentence in tokens for word in sentence)\n",
    "word_to_index = {word: i for i, word in enumerate(vocab)}\n",
    "\n",
    "# Fonction pour extraire les paires contexte-cible (Skip-Gram)\n",
    "def generate_pairs(tokens, window_size=1):\n",
    "    pairs = []\n",
    "    for sentence in tokens:\n",
    "        for i, word in enumerate(sentence):\n",
    "            target = word_to_index[word]  # Cible\n",
    "            context_words = [sentence[j] for j in range(i - window_size, i + window_size ) if j != i]\n",
    "            for context in context_words:\n",
    "                pairs.append((target, word_to_index[context]))  # (cible, contexte)\n",
    "    return pairs\n",
    "\n",
    "# Génération des paires contexte-cible\n",
    "pairs = generate_pairs(tokens, window_size=1)\n",
    "vocab_list = list(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation aléatoire des vecteurs de mots\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 10  # Taille des vecteurs d'embedding\n",
    "\n",
    "# Matrices de poids : taille (vocab_size, embedding_dim)\n",
    "W1 = np.random.rand(vocab_size, embedding_dim)  # Poids pour les cibles\n",
    "W2 = np.random.rand(vocab_size, embedding_dim)  # Poids pour les contextes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.9964132099431408\n",
      "Epoch 10, Loss: 0.5400119713758561\n",
      "Epoch 20, Loss: 0.36222910801582203\n",
      "Epoch 30, Loss: 0.26942969293131896\n",
      "Epoch 40, Loss: 0.2129356807600291\n",
      "Epoch 50, Loss: 0.17513571931186578\n",
      "Epoch 60, Loss: 0.14817262575221285\n",
      "Epoch 70, Loss: 0.12802918546287062\n",
      "Epoch 80, Loss: 0.11244433699353756\n",
      "Epoch 90, Loss: 0.1000510165625448\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Fonction de perte : log-likelihood négatif\n",
    "def loss_function(target, context, W1, W2):\n",
    "    score = np.dot(W1[target], W2[context])  # Produit scalaire\n",
    "    return -np.log(sigmoid(score))\n",
    "\n",
    "# Gradient de la perte par rapport aux poids\n",
    "def gradient(target, context, W1, W2, learning_rate=0.1):\n",
    "    score = np.dot(W1[target], W2[context])\n",
    "    prediction = sigmoid(score)\n",
    "    error = prediction - 1  # L'erreur pour une paire (cible, contexte) positive\n",
    "    grad_W1 = error * W2[context]  # Gradients pour W1 (cibles)\n",
    "    grad_W2 = error * W1[target]  # Gradients pour W2 (contextes)\n",
    "    return grad_W1, grad_W2\n",
    "\n",
    "# Entraînement : descente de gradient\n",
    "for epoch in range(100):  # Nombre d'itérations\n",
    "    total_loss = 0\n",
    "    for target, context in pairs:\n",
    "        grad_W1, grad_W2 = gradient(target, context, W1, W2)\n",
    "        W1[target] -= 0.1 * grad_W1  # Mise à jour des poids\n",
    "        W2[context] -= 0.1 * grad_W2\n",
    "        total_loss += loss_function(target, context, W1, W2)\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {total_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "le: [0.80665593 0.33176658 1.05986271 0.81997102 0.57681536 1.21649046\n",
      " 0.88467455 0.87910907 1.10978974 0.71079258]\n",
      "sous: [0.31387691 0.78417543 0.82889312 0.27291319 0.74923362 1.08794956\n",
      " 1.01249055 1.10678309 0.91419246 0.53670476]\n",
      "pomme: [0.44766915 0.72413787 0.48041782 0.30444487 0.99350513 0.15005236\n",
      " 0.52801828 0.93741134 0.28213302 1.057748  ]\n",
      "dort: [0.69846803 0.95090972 0.8930665  0.32154198 0.63480515 0.7621131\n",
      " 0.86595167 0.66995676 1.04502992 1.13044216]\n",
      "chat: [0.64022845 0.24352789 0.74688968 0.61558872 0.27848528 0.72043005\n",
      " 1.16949664 0.28555824 0.47269413 0.79113151]\n",
      "une: [1.06315688 0.34945953 0.340403   0.40011085 0.87832048 1.01584965\n",
      " 0.44408634 0.6665143  0.26676153 0.96386479]\n",
      "chien: [0.73238471 0.51687325 0.67419172 0.79377774 0.33902238 0.45904576\n",
      " 0.72771295 0.55566983 0.71006662 0.38612705]\n",
      "mange: [0.92426893 0.96817434 0.32938624 0.10810074 0.21039087 0.3170874\n",
      " 1.21621041 1.02209608 0.10278668 0.67942506]\n",
      "l'arbre: [0.71149297 0.64656014 0.23157997 0.13588644 0.05076856 1.11963087\n",
      " 0.88216544 1.43843064 0.11886895 0.68625539]\n",
      "aussi: [0.97061948 0.56835735 0.51648939 0.24374064 1.00758525 0.79586921\n",
      " 0.14192971 0.96917019 0.52796822 0.55611394]\n"
     ]
    }
   ],
   "source": [
    "# Vérification des vecteurs de mots\n",
    "for word, index in word_to_index.items():\n",
    "    print(f\"{word}: {W1[index]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18. What is the difference between static and contextual embeddings?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **Static Embeddings (Représentations statiques)** :\n",
    "- **Vecteur fixe** : Chaque mot a un vecteur unique, quelle que soit sa signification ou son contexte.\n",
    "- **Exemples** : Word2Vec, GloVe, FastText.\n",
    "- **Avantage** : Simples et rapides à utiliser.\n",
    "- **Limite** : Ne distinguent pas les mots à significations multiples (e.g., \"bank\" = banque/rive).\n",
    "\n",
    "---\n",
    "\n",
    "### **Contextual Embeddings (Représentations contextuelles)** :\n",
    "- **Vecteur dynamique** : Le vecteur d’un mot change en fonction du contexte.\n",
    "- **Exemples** : BERT, GPT, ELMo.\n",
    "- **Avantage** : Capturent les différentes significations d’un mot selon son contexte.\n",
    "- **Limite** : Plus coûteux en calcul.\n",
    "\n",
    "---\n",
    "\n",
    "### **Résumé des usages** :\n",
    "- **Statique** : Tâches simples où le contexte importe peu.\n",
    "- **Contextuel** : Tâches complexes nécessitant une compréhension fine du langage (traduction, résolution d’ambiguïté)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 22. What are dense and sparse embeddings? Provide examples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Différence entre Sparse et Dense Embeddings :**\n",
    "\n",
    "1. **Sparse Embeddings (Représentation Creuse) :**\n",
    "   - **Caractéristique :** Grande dimension avec principalement des zéros.\n",
    "   - **Exemple :** One-hot encoding pour \"chien\" → `[0, 1, 0, 0]`.\n",
    "   - **Limites :** \n",
    "     - Pas de sens sémantique.\n",
    "     - Inefficace en mémoire et calcul.\n",
    "\n",
    "2. **Dense Embeddings (Représentation Dense) :**\n",
    "   - **Caractéristique :** Faible dimension avec des valeurs riches et non nulles.\n",
    "   - **Exemple :** Word2Vec pour \"chien\" → `[0.8, 0.5, -0.3]`.\n",
    "   - **Avantages :** \n",
    "     - Capturent des relations sémantiques (proximité entre \"chien\" et \"chat\").\n",
    "     - Compactes et efficaces.\n",
    "\n",
    "---\n",
    "\n",
    "### **Résumé :**\n",
    "- Sparse embeddings sont simples mais inefficaces et sans relations sémantiques.\n",
    "- Dense embeddings sont compacts, efficaces et capturent le sens et les similitudes, largement utilisés dans les modèles modernes comme Word2Vec et BERT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20. What is the difference between Glove and Word2Vec ?\n",
    "\n",
    "**Word2Vec utilise une fenêtre de contexte (généralement petite) pour apprendre des relations entre les mots. Cela permet de capturer des associations locales entre les mots qui apparaissent fréquemment dans un même contexte.**\n",
    "**Limite : Ce processus néglige les relations globales et les co-occurrences entre mots dans l'ensemble du corpus.**\n",
    "**---> GloVe : Capture des relations globales en utilisant la matrice de co-occurrence sur l'ensemble du corpus.**\n",
    "\n",
    "## 21. What is negative sampling and why is it needed? What other tricks for Word2Vec do you know, and how can you apply them?\n",
    "\n",
    "**Negative Sampling : Accélère l'entraînement en mettant à jour un sous-ensemble de mots négatifs au lieu de tous les mots.**\n",
    "\n",
    "## 23. Why might the dimensionality of embeddings be important?\n",
    "\n",
    "**La dimensionnalité des embeddings doit être choisie en fonction de la complexité des données, de la taille du vocabulaire et des besoins des modèles aval. Un bon choix équilibre précision, efficacité et généralisation.**\n",
    "\n",
    "## 24. What problems can arise when training Word2Vec on short textual data, and how can you deal with them?\n",
    "\n",
    "**Lorsque vous entraînez Word2Vec sur des données courtes, vous devez gérer la limitation du vocabulaire, le risque de sur-ajustement et la faible richesse du contexte. Utiliser des techniques d'enrichissement de données, de régularisation et de pré-traitement peut aider à améliorer les performances du modèle.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
