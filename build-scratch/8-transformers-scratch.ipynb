{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ATTENTION AND TRANSFORMER ARCHITECTURE (15)\n",
    "\n",
    "32. How do you compute attention ? (additional: for what task was it proposed, and why?)\n",
    "\n",
    "33. Complexity of attention? Compare it with the complexity in RNN.\n",
    "\n",
    "34. Compare RNN and attention . In what cases would you use attention, and when RNN?\n",
    "\n",
    "35. Write attention from scratch.\n",
    "\n",
    "36. Explain masking in attention.\n",
    "\n",
    "37. What is the dimensionality of the self-attention matrix?\n",
    "\n",
    "38. What is the difference between BERT and GPT in terms of attention calculation ?\n",
    "\n",
    "39. What is the dimensionality of the embedding layer in the transformer?\n",
    "\n",
    "40. Why are embeddings called contextual? How does it work?\n",
    "\n",
    "41. What is used in transformers, layer norm or batch norm , and why?\n",
    "\n",
    "42. Why do transformers have PreNorm and PostNorm ?\n",
    "\n",
    "43. Explain the difference between soft and hard (local/global) attention?\n",
    "\n",
    "44. Explain multihead attention.\n",
    "\n",
    "45. What other types of attention mechanisms do you know? What are the purposes of these modifications?\n",
    "\n",
    "46. How does self-attention become more complex with an increase in the number of heads?"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
