{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Build RNN from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### √âtape 1 : Pr√©parer les donn√©es\n",
    "- Collecte de texte : Choisissez un texte √† utiliser comme donn√©es d'entra√Ænement (par exemple, un roman ou une collection de po√®mes).\n",
    "Traitement des donn√©es :\n",
    "- Convertir les caract√®res en entiers (indexation des caract√®res)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ': 0,\n",
       " '.': 1,\n",
       " 'N': 2,\n",
       " 'R': 3,\n",
       " 'a': 4,\n",
       " 'd': 5,\n",
       " 'e': 6,\n",
       " 'l': 7,\n",
       " 'm': 8,\n",
       " 'n': 9,\n",
       " 'o': 10,\n",
       " 'p': 11,\n",
       " 'r': 12,\n",
       " 't': 13,\n",
       " 'u': 14,\n",
       " 'x': 15,\n",
       " '√Æ': 16}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"exemple de texte pour entra√Æner un RNN.\"  # Donn√©es\n",
    "chars = sorted(list(set(text)))\n",
    "char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
    "idx_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "char_to_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 2 - **Cr√©er des s√©quences d'entr√©e-sortie (par exemple, une s√©quence de caract√®res comme entr√©e et le caract√®re suivant comme sortie).**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entr√©e: ['exemp', 'xempl', 'emple', 'mple ', 'ple d']\n",
      "Sortie: ['l', 'e', ' ', 'd', 'e']\n"
     ]
    }
   ],
   "source": [
    "seq_length = 5  # Longueur de la s√©quence\n",
    "inputs = []  # Contiendra les s√©quences d'entr√©e\n",
    "targets = []  # Contiendra les sorties correspondantes\n",
    "\n",
    "for i in range(len(text) - seq_length):\n",
    "    inputs.append(text[i:i + seq_length])  # S√©quence de 5 caract√®res\n",
    "    targets.append(text[i + seq_length])  # Caract√®re suivant\n",
    "\n",
    "# Exemple de r√©sultat\n",
    "print(\"Entr√©e:\", inputs[:5])\n",
    "print(\"Sortie:\", targets[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 3- **One hot encoding de chaque caract√©re**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ': [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " '.': [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " 'N': [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " 'R': [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " 'a': [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " 'd': [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " 'e': [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " 'l': [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " 'm': [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " 'n': [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       " 'o': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       " 'p': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       " 'r': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       " 't': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       " 'u': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " 'x': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       " '√Æ': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def one_hot_dict(vocab_size:int, chars:list):\n",
    "    \"\"\"One-hot encoding of a each character in the list.\"\"\"\n",
    "    dict_one = {}\n",
    "    for i, char in enumerate(chars):\n",
    "        encoding = [0] * vocab_size\n",
    "        encoding[i] = 1\n",
    "        dict_one[char] = encoding\n",
    "    return dict_one\n",
    "\n",
    "one_hot_dict(len(chars), chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **3 : Initialiser les param√®tres du RNN**\n",
    "\n",
    "Un RNN utilise des matrices de poids pour transformer les donn√©es d'entr√©e et les √©tats cach√©s en sorties. Voici les **√©l√©ments principaux** qu'on doit initialiser :\n",
    "\n",
    "1. **Poids d'entr√©e vers l'√©tat cach√© (\\(W_{xh}\\))** :\n",
    "   - Transforme l'entr√©e (vecteur one-hot) en une contribution √† l'√©tat cach√©.\n",
    "\n",
    "2. **Poids de r√©currence (\\(W_{hh}\\))** :\n",
    "   - Combine l'√©tat cach√© pr√©c√©dent pour calculer le nouvel √©tat cach√©.\n",
    "\n",
    "3. **Poids de l'√©tat cach√© vers la sortie (\\(W_{hy}\\))** :\n",
    "   - Transforme l'√©tat cach√© actuel en une probabilit√© pour chaque caract√®re (softmax).\n",
    "\n",
    "4. **Biais (\\(b_h\\) et \\(b_y\\))** :\n",
    "   - Valeurs ajout√©es respectivement √† l'√©tat cach√© et √† la sortie pour ajuster les calculs.\n",
    "\n",
    "5. **√âtat cach√© initial (\\(h_0\\))** :\n",
    "   - D√©bute g√©n√©ralement avec un vecteur nul.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "hidden_size = 50\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# Initialisation des poids\n",
    "W_xh = np.random.randn(hidden_size, vocab_size) * 0.01\n",
    "W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n",
    "W_hy = np.random.randn(vocab_size, hidden_size) * 0.01\n",
    "\n",
    "# Initialisation des biais\n",
    "b_h = np.zeros((hidden_size, 1))\n",
    "b_y = np.zeros((vocab_size, 1))\n",
    "\n",
    "# √âtat cach√© initial\n",
    "hprev = np.zeros((hidden_size, 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bien jou√© pour l'initialisation‚ÄØ! Maintenant, nous passons √† **l'√©tape cl√©** : **la propagation vers l‚Äôavant (forward pass)**.\n",
    "\n",
    "---\n",
    "\n",
    "### **√âtape 4 : Propagation vers l‚Äôavant**\n",
    "\n",
    "#### **Objectif**\n",
    "Calculer la sortie du RNN pour une s√©quence donn√©e :\n",
    "1. Prendre chaque caract√®re de la s√©quence d'entr√©e (one-hot encod√©).\n",
    "2. Mettre √† jour l'√©tat cach√© (\\(h_t\\)) en fonction de l'entr√©e actuelle et de l'√©tat pr√©c√©dent.\n",
    "3. Calculer la probabilit√© de sortie (\\(y_t\\)) pour pr√©dire le prochain caract√®re.\n",
    "\n",
    "#### **Pourquoi ?**\n",
    "La propagation vers l‚Äôavant permet au RNN de :\n",
    "- Combiner les informations pass√©es (via \\(h_t\\)).\n",
    "- Produire une probabilit√© pour chaque caract√®re du vocabulaire en sortie.\n",
    "\n",
    "---\n",
    "\n",
    "### **Formules principales**\n",
    "1. **Mise √† jour de l'√©tat cach√©** :\n",
    "   \\[$\n",
    "   h_t = \\tanh(W_{xh} \\cdot x_t + W_{hh} \\cdot h_{t-1} + b_h)\n",
    "   $\\]\n",
    "\n",
    "2. **Calcul de la sortie** :\n",
    "   \\[$\n",
    "   y_t = \\text{softmax}(W_{hy} \\cdot h_t + b_y)\n",
    "   $\\]\n",
    "\n",
    "3. **Softmax** (convertir les scores en probabilit√©s) :\n",
    "   \\[$\n",
    "   y_t[i] = \\frac{e^{z[i]}}{\\sum_{j} e^{z[j]}}\n",
    "   $\\]\n",
    "\n",
    "---\n",
    "\n",
    "### **√âtape √† suivre pour coder cela**\n",
    "1. **Initialiser l'√©tat cach√©** (\\(h_0\\)) avec `hprev`.\n",
    "2. **Pour chaque caract√®re** dans la s√©quence d'entr√©e :\n",
    "   - Multiplier \\(x_t\\) par \\(W_{xh}\\).\n",
    "   - Ajouter la contribution de l'√©tat pr√©c√©dent (\\(W_{hh} \\cdot h_{t-1}\\)).\n",
    "   - Ajouter le biais (\\(b_h\\)) et appliquer `tanh` pour obtenir \\(h_t\\).\n",
    "   - Calculer la sortie brute (\\(W_{hy} \\cdot h_t + b_y\\)).\n",
    "   - Appliquer la softmax pour obtenir les probabilit√©s.\n",
    "\n",
    "3. **Retourner les √©tats cach√©s (\\(h_t\\)) et les probabilit√©s (\\(y_t\\))**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(inputs, hprev, W_xh, W_hh, W_hy, b_h, b_y, vocab_size, one_hot_dict):\n",
    "    h = hprev  # √âtat cach√© initial\n",
    "    outputs = []  # Stocker les probabilit√©s de sortie\n",
    "    hs = {}  # Stocker les √©tats cach√©s pour chaque √©tape\n",
    "    hs[-1] = np.copy(h)  # √âtat cach√© initial\n",
    "    \n",
    "    for t, char in enumerate(inputs):\n",
    "        # Encoder le caract√®re en one-hot\n",
    "        x_t = np.array(one_hot_dict[char]).reshape(-1, 1)\n",
    "        \n",
    "        # Calculer le nouvel √©tat cach√©\n",
    "        h = np.tanh(np.dot(W_xh, x_t) + np.dot(W_hh, h) + b_h)\n",
    "        hs[t] = np.copy(h)\n",
    "        \n",
    "        # Calculer la sortie brute et appliquer softmax\n",
    "        y_raw = np.dot(W_hy, h) + b_y\n",
    "        y = np.exp(y_raw) / np.sum(np.exp(y_raw))  # Softmax\n",
    "        outputs.append(y)\n",
    "    \n",
    "    return outputs, hs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une fois la propagation vers l'avant impl√©ment√©e et test√©e, l'√©tape suivante est d'impl√©menter la **propagation arri√®re** (**Backpropagation Through Time, ou BPTT**) pour permettre au RNN d'apprendre √† partir des erreurs. C'est l√† que le RNN ajuste ses poids et biais pour mieux pr√©dire √† l'avenir.\n",
    "\n",
    "---\n",
    "\n",
    "### **√âtape 5 : Propagation arri√®re (BPTT)**\n",
    "\n",
    "#### **Pourquoi ?**\n",
    "La propagation arri√®re permet de calculer les gradients des pertes par rapport aux param√®tres du mod√®le (\\(W_{xh}\\), \\(W_{hh}\\), \\(W_{hy}\\), \\(b_h\\), \\(b_y\\)) afin d'effectuer une mise √† jour avec un algorithme comme la descente de gradient.\n",
    "\n",
    "#### **D√©roulement de la BPTT**\n",
    "1. **Calculer la perte globale** :\n",
    "   - Pour chaque √©tape, compare les sorties pr√©dites (\\(y_t\\)) aux cibles r√©elles (c'est-√†-dire le caract√®re suivant attendu).\n",
    "   - Utilise une fonction de perte, par exemple, **l'entropie crois√©e** :\n",
    "     \\[\n",
    "     \\text{loss} = -\\sum_t \\log(y_t[cible])\n",
    "     \\]\n",
    "\n",
    "2. **Calculer les gradients** :\n",
    "   - Remonte √† travers le temps (de \\(T\\) √† \\(0\\)) pour calculer les contributions de chaque √©tape √† la perte globale.\n",
    "   - Propager les gradients des erreurs dans :\n",
    "     - Les sorties (\\(y_t\\)).\n",
    "     - L'√©tat cach√© (\\(h_t\\)).\n",
    "     - Les poids (\\(W_{xh}\\), \\(W_{hh}\\), \\(W_{hy}\\)) et les biais.\n",
    "\n",
    "3. **Mettre √† jour les param√®tres** :\n",
    "   - Applique les gradients pour ajuster les poids et les biais via la **descente de gradient** :\n",
    "     \\[\n",
    "     \\theta \\leftarrow \\theta - \\eta \\cdot \\frac{\\partial \\text{loss}}{\\partial \\theta}\n",
    "     \\]\n",
    "   - (\\(\\eta\\) est le taux d'apprentissage).\n",
    "\n",
    "---\n",
    "\n",
    "### **√âtapes pour le code**\n",
    "1. **Calculer la perte** :\n",
    "   Impl√©mente la formule d'entropie crois√©e pour chaque √©tape.\n",
    "\n",
    "2. **R√©tropropager les gradients** :\n",
    "   Calcule les d√©riv√©es partielles pour chaque param√®tre :\n",
    "   - Les poids d'entr√©e (\\(W_{xh}\\)).\n",
    "   - Les poids r√©currents (\\(W_{hh}\\)).\n",
    "   - Les poids de sortie (\\(W_{hy}\\)).\n",
    "   - Les biais (\\(b_h\\), \\(b_y\\)).\n",
    "\n",
    "3. **Effectuer une mise √† jour des param√®tres**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_pass(inputs, targets, outputs, hs, W_xh, W_hh, W_hy, b_h, b_y, vocab_size, one_hot_dict, learning_rate):\n",
    "    # Initialiser les gradients\n",
    "    dW_xh, dW_hh, dW_hy = np.zeros_like(W_xh), np.zeros_like(W_hh), np.zeros_like(W_hy)\n",
    "    db_h, db_y = np.zeros_like(b_h), np.zeros_like(b_y)\n",
    "    dh_next = np.zeros_like(hs[0])  # Gradient de l'√©tat cach√© suivant\n",
    "    \n",
    "    # Boucle inverse (du dernier caract√®re au premier)\n",
    "    for t in reversed(range(len(inputs))):\n",
    "        # Calcul de l'erreur de sortie\n",
    "        dy = np.copy(outputs[t])\n",
    "        target_idx = char_to_idx[targets[t]]\n",
    "        dy[target_idx] -= 1  # Gradient de la perte par rapport √† la sortie\n",
    "        \n",
    "        # Gradient pour W_hy et b_y\n",
    "        dW_hy += np.dot(dy, hs[t].T)\n",
    "        db_y += dy\n",
    "        \n",
    "        # Gradient de l'√©tat cach√©\n",
    "        dh = np.dot(W_hy.T, dy) + dh_next  # Gradient total pour h_t\n",
    "        dh_raw = (1 - hs[t] ** 2) * dh  # Gradient apr√®s tanh\n",
    "        \n",
    "        # Gradient pour W_xh, W_hh, et b_h\n",
    "        dW_xh += np.dot(dh_raw, one_hot_dict[inputs[t]].reshape(1, -1))\n",
    "        dW_hh += np.dot(dh_raw, hs[t - 1].T)\n",
    "        db_h += dh_raw\n",
    "        \n",
    "        # Propager le gradient vers l'√©tat cach√© pr√©c√©dent\n",
    "        dh_next = np.dot(W_hh.T, dh_raw)\n",
    "    \n",
    "    # Clipping des gradients (pour √©viter exploding gradients)\n",
    "    for dparam in [dW_xh, dW_hh, dW_hy, db_h, db_y]:\n",
    "        np.clip(dparam, -5, 5, out=dparam)\n",
    "    \n",
    "    # Mise √† jour des param√®tres\n",
    "    W_xh -= learning_rate * dW_xh\n",
    "    W_hh -= learning_rate * dW_hh\n",
    "    W_hy -= learning_rate * dW_hy\n",
    "    b_h -= learning_rate * db_h\n",
    "    b_y -= learning_rate * db_y\n",
    "    \n",
    "    return W_xh, W_hh, W_hy, b_h, b_y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entra√Ænement du mod√®le\n",
    "L'entra√Ænement consiste √† it√©rer plusieurs fois sur les donn√©es (les √©poques), appliquer la propagation avant pour obtenir les sorties du RNN, calculer la perte, et mettre √† jour les poids avec la r√©tropropagation.\n",
    "\n",
    "- **Objectif** :\n",
    "\n",
    "Entra√Æner le RNN sur plusieurs √©poques (par exemple, 100 √©poques).\n",
    "√Ä chaque √©poque, passer par toutes les s√©quences de texte, calculer la perte et ajuster les poids.\n",
    "- **Pourquoi ?**\n",
    "L'entra√Ænement permet au RNN d'am√©liorer ses pr√©dictions en ajustant progressivement ses param√®tres (poids et biais). Chaque √©poque aide le mod√®le √† mieux comprendre la structure du texte pour g√©n√©rer des s√©quences plus coh√©rentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rnn(inputs, targets, vocab_size, one_hot_dict, n_epochs, learning_rate):\n",
    "    W_xh, W_hh, W_hy, b_h, b_y = initialize_parameters(vocab_size)  # Initialise les param√®tres\n",
    "    for epoch in range(n_epochs):\n",
    "        loss = 0  # Pour suivre la perte de chaque √©poque\n",
    "        hprev = np.zeros((hidden_size, 1))  # R√©initialise l'√©tat cach√© pour chaque √©poque\n",
    "        \n",
    "        for i in range(len(inputs)):\n",
    "            # Propagation avant\n",
    "            outputs, hs = forward_pass(inputs[i], hprev, W_xh, W_hh, W_hy, b_h, b_y, vocab_size, one_hot_dict)\n",
    "            \n",
    "            # Calcul de la perte (entropie crois√©e)\n",
    "            target_idx = char_to_idx[targets[i]]\n",
    "            loss += -np.log(outputs[-1][target_idx])\n",
    "            \n",
    "            # R√©tropropagation\n",
    "            W_xh, W_hh, W_hy, b_h, b_y = backward_pass(inputs[i], targets[i], outputs, hs, W_xh, W_hh, W_hy, b_h, b_y, vocab_size, one_hot_dict, learning_rate)\n",
    "        \n",
    "        # Affichage de la perte √† chaque √©poque\n",
    "        print(f'√âpoque {epoch+1}/{n_epochs}, Perte : {loss}')\n",
    "    \n",
    "    return W_xh, W_hh, W_hy, b_h, b_y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## G√©n√©ration de texte\n",
    "Une fois que le mod√®le est entra√Æn√©, tu peux l'utiliser pour g√©n√©rer du texte √† partir d'un caract√®re initial.\n",
    "\n",
    "Objectif : G√©n√©rer une s√©quence de texte de longueur ùêø\n",
    "L en partant d'un caract√®re initial et en utilisant les sorties du RNN pour pr√©dire le caract√®re suivant.\n",
    "\n",
    "Pourquoi ?\n",
    "La g√©n√©ration de texte permet de tester la capacit√© du mod√®le √† apprendre des d√©pendances et √† pr√©dire des caract√®res coh√©rents √† partir du texte appris.\n",
    "\n",
    "Comment ?\n",
    "Initialiser l'√©tat cach√© avec un vecteur nul ou un √©tat cach√© appris.\n",
    "Donner un caract√®re initial comme entr√©e.\n",
    "√Ä chaque √©tape :\n",
    "Pr√©dire le prochain caract√®re en fonction de l'√©tat cach√©.\n",
    "Utiliser ce caract√®re comme entr√©e pour la pr√©diction suivante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(start_char, length, W_xh, W_hh, W_hy, b_h, b_y, one_hot_dict, idx_to_char):\n",
    "    hprev = np.zeros((hidden_size, 1))  # Initialisation de l'√©tat cach√©\n",
    "    generated_text = start_char  # Commencer avec le caract√®re initial\n",
    "    \n",
    "    for i in range(length):\n",
    "        x_t = np.array(one_hot_dict[start_char]).reshape(-1, 1)  # Encoder le caract√®re initial\n",
    "        h = np.tanh(np.dot(W_xh, x_t) + np.dot(W_hh, hprev) + b_h)  # Propagation avant\n",
    "        \n",
    "        # Calcul de la sortie et application de softmax\n",
    "        y_raw = np.dot(W_hy, h) + b_y\n",
    "        y = np.exp(y_raw) / np.sum(np.exp(y_raw))  # Softmax\n",
    "        \n",
    "        # Choisir le caract√®re suivant (maximum de la probabilit√©)\n",
    "        next_char_idx = np.argmax(y)\n",
    "        next_char = idx_to_char[next_char_idx]\n",
    "        \n",
    "        # Ajouter le caract√®re √† la s√©quence g√©n√©r√©e\n",
    "        generated_text += next_char\n",
    "        start_char = next_char  # Utiliser le prochain caract√®re comme entr√©e pour la suivante g√©n√©ration\n",
    "        hprev = h  # Mettre √† jour l'√©tat cach√©\n",
    "    \n",
    "    return generated_text\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
