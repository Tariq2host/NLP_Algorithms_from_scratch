{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Build RNN from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Étape 1 : Préparer les données\n",
    "- Collecte de texte : Choisissez un texte à utiliser comme données d'entraînement (par exemple, un roman ou une collection de poèmes).\n",
    "Traitement des données :\n",
    "- Convertir les caractères en entiers (indexation des caractères)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ': 0,\n",
       " '.': 1,\n",
       " 'N': 2,\n",
       " 'R': 3,\n",
       " 'a': 4,\n",
       " 'd': 5,\n",
       " 'e': 6,\n",
       " 'l': 7,\n",
       " 'm': 8,\n",
       " 'n': 9,\n",
       " 'o': 10,\n",
       " 'p': 11,\n",
       " 'r': 12,\n",
       " 't': 13,\n",
       " 'u': 14,\n",
       " 'x': 15,\n",
       " 'î': 16}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"exemple de texte pour entraîner un RNN.\"  # Données\n",
    "chars = sorted(list(set(text)))\n",
    "char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
    "idx_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "char_to_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 2 - **Créer des séquences d'entrée-sortie (par exemple, une séquence de caractères comme entrée et le caractère suivant comme sortie).**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrée: ['exemp', 'xempl', 'emple', 'mple ', 'ple d']\n",
      "Sortie: ['l', 'e', ' ', 'd', 'e']\n"
     ]
    }
   ],
   "source": [
    "seq_length = 5  # Longueur de la séquence\n",
    "inputs = []  # Contiendra les séquences d'entrée\n",
    "targets = []  # Contiendra les sorties correspondantes\n",
    "\n",
    "for i in range(len(text) - seq_length):\n",
    "    inputs.append(text[i:i + seq_length])  # Séquence de 5 caractères\n",
    "    targets.append(text[i + seq_length])  # Caractère suivant\n",
    "\n",
    "# Exemple de résultat\n",
    "print(\"Entrée:\", inputs[:5])\n",
    "print(\"Sortie:\", targets[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 3- **One hot encoding de chaque caractére**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ': [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " '.': [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " 'N': [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " 'R': [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " 'a': [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " 'd': [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " 'e': [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " 'l': [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " 'm': [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " 'n': [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       " 'o': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       " 'p': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       " 'r': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       " 't': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       " 'u': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " 'x': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       " 'î': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def one_hot_dict(vocab_size:int, chars:list):\n",
    "    \"\"\"One-hot encoding of a each character in the list.\"\"\"\n",
    "    dict_one = {}\n",
    "    for i, char in enumerate(chars):\n",
    "        encoding = [0] * vocab_size\n",
    "        encoding[i] = 1\n",
    "        dict_one[char] = encoding\n",
    "    return dict_one\n",
    "\n",
    "one_hot_dict(len(chars), chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **3 : Initialiser les paramètres du RNN**\n",
    "\n",
    "Un RNN utilise des matrices de poids pour transformer les données d'entrée et les états cachés en sorties. Voici les **éléments principaux** qu'on doit initialiser :\n",
    "\n",
    "1. **Poids d'entrée vers l'état caché (\\(W_{xh}\\))** :\n",
    "   - Transforme l'entrée (vecteur one-hot) en une contribution à l'état caché.\n",
    "\n",
    "2. **Poids de récurrence (\\(W_{hh}\\))** :\n",
    "   - Combine l'état caché précédent pour calculer le nouvel état caché.\n",
    "\n",
    "3. **Poids de l'état caché vers la sortie (\\(W_{hy}\\))** :\n",
    "   - Transforme l'état caché actuel en une probabilité pour chaque caractère (softmax).\n",
    "\n",
    "4. **Biais (\\(b_h\\) et \\(b_y\\))** :\n",
    "   - Valeurs ajoutées respectivement à l'état caché et à la sortie pour ajuster les calculs.\n",
    "\n",
    "5. **État caché initial (\\(h_0\\))** :\n",
    "   - Débute généralement avec un vecteur nul.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "hidden_size = 50\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# Initialisation des poids\n",
    "W_xh = np.random.randn(hidden_size, vocab_size) * 0.01\n",
    "W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n",
    "W_hy = np.random.randn(vocab_size, hidden_size) * 0.01\n",
    "\n",
    "# Initialisation des biais\n",
    "b_h = np.zeros((hidden_size, 1))\n",
    "b_y = np.zeros((vocab_size, 1))\n",
    "\n",
    "# État caché initial\n",
    "hprev = np.zeros((hidden_size, 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bien joué pour l'initialisation ! Maintenant, nous passons à **l'étape clé** : **la propagation vers l’avant (forward pass)**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Étape 4 : Propagation vers l’avant**\n",
    "\n",
    "#### **Objectif**\n",
    "Calculer la sortie du RNN pour une séquence donnée :\n",
    "1. Prendre chaque caractère de la séquence d'entrée (one-hot encodé).\n",
    "2. Mettre à jour l'état caché (\\(h_t\\)) en fonction de l'entrée actuelle et de l'état précédent.\n",
    "3. Calculer la probabilité de sortie (\\(y_t\\)) pour prédire le prochain caractère.\n",
    "\n",
    "#### **Pourquoi ?**\n",
    "La propagation vers l’avant permet au RNN de :\n",
    "- Combiner les informations passées (via \\(h_t\\)).\n",
    "- Produire une probabilité pour chaque caractère du vocabulaire en sortie.\n",
    "\n",
    "---\n",
    "\n",
    "### **Formules principales**\n",
    "1. **Mise à jour de l'état caché** :\n",
    "   \\[$\n",
    "   h_t = \\tanh(W_{xh} \\cdot x_t + W_{hh} \\cdot h_{t-1} + b_h)\n",
    "   $\\]\n",
    "\n",
    "2. **Calcul de la sortie** :\n",
    "   \\[$\n",
    "   y_t = \\text{softmax}(W_{hy} \\cdot h_t + b_y)\n",
    "   $\\]\n",
    "\n",
    "3. **Softmax** (convertir les scores en probabilités) :\n",
    "   \\[$\n",
    "   y_t[i] = \\frac{e^{z[i]}}{\\sum_{j} e^{z[j]}}\n",
    "   $\\]\n",
    "\n",
    "---\n",
    "\n",
    "### **Étape à suivre pour coder cela**\n",
    "1. **Initialiser l'état caché** (\\(h_0\\)) avec `hprev`.\n",
    "2. **Pour chaque caractère** dans la séquence d'entrée :\n",
    "   - Multiplier \\(x_t\\) par \\(W_{xh}\\).\n",
    "   - Ajouter la contribution de l'état précédent (\\(W_{hh} \\cdot h_{t-1}\\)).\n",
    "   - Ajouter le biais (\\(b_h\\)) et appliquer `tanh` pour obtenir \\(h_t\\).\n",
    "   - Calculer la sortie brute (\\(W_{hy} \\cdot h_t + b_y\\)).\n",
    "   - Appliquer la softmax pour obtenir les probabilités.\n",
    "\n",
    "3. **Retourner les états cachés (\\(h_t\\)) et les probabilités (\\(y_t\\))**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(inputs, hprev, W_xh, W_hh, W_hy, b_h, b_y, vocab_size, one_hot_dict):\n",
    "    h = hprev  # État caché initial\n",
    "    outputs = []  # Stocker les probabilités de sortie\n",
    "    hs = {}  # Stocker les états cachés pour chaque étape\n",
    "    hs[-1] = np.copy(h)  # État caché initial\n",
    "    \n",
    "    for t, char in enumerate(inputs):\n",
    "        # Encoder le caractère en one-hot\n",
    "        x_t = np.array(one_hot_dict[char]).reshape(-1, 1)\n",
    "        \n",
    "        # Calculer le nouvel état caché\n",
    "        h = np.tanh(np.dot(W_xh, x_t) + np.dot(W_hh, h) + b_h)\n",
    "        hs[t] = np.copy(h)\n",
    "        \n",
    "        # Calculer la sortie brute et appliquer softmax\n",
    "        y_raw = np.dot(W_hy, h) + b_y\n",
    "        y = np.exp(y_raw) / np.sum(np.exp(y_raw))  # Softmax\n",
    "        outputs.append(y)\n",
    "    \n",
    "    return outputs, hs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une fois la propagation vers l'avant implémentée et testée, l'étape suivante est d'implémenter la **propagation arrière** (**Backpropagation Through Time, ou BPTT**) pour permettre au RNN d'apprendre à partir des erreurs. C'est là que le RNN ajuste ses poids et biais pour mieux prédire à l'avenir.\n",
    "\n",
    "---\n",
    "\n",
    "### **Étape 5 : Propagation arrière (BPTT)**\n",
    "\n",
    "#### **Pourquoi ?**\n",
    "La propagation arrière permet de calculer les gradients des pertes par rapport aux paramètres du modèle (\\(W_{xh}\\), \\(W_{hh}\\), \\(W_{hy}\\), \\(b_h\\), \\(b_y\\)) afin d'effectuer une mise à jour avec un algorithme comme la descente de gradient.\n",
    "\n",
    "#### **Déroulement de la BPTT**\n",
    "1. **Calculer la perte globale** :\n",
    "   - Pour chaque étape, compare les sorties prédites (\\(y_t\\)) aux cibles réelles (c'est-à-dire le caractère suivant attendu).\n",
    "   - Utilise une fonction de perte, par exemple, **l'entropie croisée** :\n",
    "     \\[\n",
    "     \\text{loss} = -\\sum_t \\log(y_t[cible])\n",
    "     \\]\n",
    "\n",
    "2. **Calculer les gradients** :\n",
    "   - Remonte à travers le temps (de \\(T\\) à \\(0\\)) pour calculer les contributions de chaque étape à la perte globale.\n",
    "   - Propager les gradients des erreurs dans :\n",
    "     - Les sorties (\\(y_t\\)).\n",
    "     - L'état caché (\\(h_t\\)).\n",
    "     - Les poids (\\(W_{xh}\\), \\(W_{hh}\\), \\(W_{hy}\\)) et les biais.\n",
    "\n",
    "3. **Mettre à jour les paramètres** :\n",
    "   - Applique les gradients pour ajuster les poids et les biais via la **descente de gradient** :\n",
    "     \\[\n",
    "     \\theta \\leftarrow \\theta - \\eta \\cdot \\frac{\\partial \\text{loss}}{\\partial \\theta}\n",
    "     \\]\n",
    "   - (\\(\\eta\\) est le taux d'apprentissage).\n",
    "\n",
    "---\n",
    "\n",
    "### **Étapes pour le code**\n",
    "1. **Calculer la perte** :\n",
    "   Implémente la formule d'entropie croisée pour chaque étape.\n",
    "\n",
    "2. **Rétropropager les gradients** :\n",
    "   Calcule les dérivées partielles pour chaque paramètre :\n",
    "   - Les poids d'entrée (\\(W_{xh}\\)).\n",
    "   - Les poids récurrents (\\(W_{hh}\\)).\n",
    "   - Les poids de sortie (\\(W_{hy}\\)).\n",
    "   - Les biais (\\(b_h\\), \\(b_y\\)).\n",
    "\n",
    "3. **Effectuer une mise à jour des paramètres**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_pass(inputs, targets, outputs, hs, W_xh, W_hh, W_hy, b_h, b_y, vocab_size, one_hot_dict, learning_rate):\n",
    "    # Initialiser les gradients\n",
    "    dW_xh, dW_hh, dW_hy = np.zeros_like(W_xh), np.zeros_like(W_hh), np.zeros_like(W_hy)\n",
    "    db_h, db_y = np.zeros_like(b_h), np.zeros_like(b_y)\n",
    "    dh_next = np.zeros_like(hs[0])  # Gradient de l'état caché suivant\n",
    "    \n",
    "    # Boucle inverse (du dernier caractère au premier)\n",
    "    for t in reversed(range(len(inputs))):\n",
    "        # Calcul de l'erreur de sortie\n",
    "        dy = np.copy(outputs[t])\n",
    "        target_idx = char_to_idx[targets[t]]\n",
    "        dy[target_idx] -= 1  # Gradient de la perte par rapport à la sortie\n",
    "        \n",
    "        # Gradient pour W_hy et b_y\n",
    "        dW_hy += np.dot(dy, hs[t].T)\n",
    "        db_y += dy\n",
    "        \n",
    "        # Gradient de l'état caché\n",
    "        dh = np.dot(W_hy.T, dy) + dh_next  # Gradient total pour h_t\n",
    "        dh_raw = (1 - hs[t] ** 2) * dh  # Gradient après tanh\n",
    "        \n",
    "        # Gradient pour W_xh, W_hh, et b_h\n",
    "        dW_xh += np.dot(dh_raw, one_hot_dict[inputs[t]].reshape(1, -1))\n",
    "        dW_hh += np.dot(dh_raw, hs[t - 1].T)\n",
    "        db_h += dh_raw\n",
    "        \n",
    "        # Propager le gradient vers l'état caché précédent\n",
    "        dh_next = np.dot(W_hh.T, dh_raw)\n",
    "    \n",
    "    # Clipping des gradients (pour éviter exploding gradients)\n",
    "    for dparam in [dW_xh, dW_hh, dW_hy, db_h, db_y]:\n",
    "        np.clip(dparam, -5, 5, out=dparam)\n",
    "    \n",
    "    # Mise à jour des paramètres\n",
    "    W_xh -= learning_rate * dW_xh\n",
    "    W_hh -= learning_rate * dW_hh\n",
    "    W_hy -= learning_rate * dW_hy\n",
    "    b_h -= learning_rate * db_h\n",
    "    b_y -= learning_rate * db_y\n",
    "    \n",
    "    return W_xh, W_hh, W_hy, b_h, b_y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entraînement du modèle\n",
    "L'entraînement consiste à itérer plusieurs fois sur les données (les époques), appliquer la propagation avant pour obtenir les sorties du RNN, calculer la perte, et mettre à jour les poids avec la rétropropagation.\n",
    "\n",
    "- **Objectif** :\n",
    "\n",
    "Entraîner le RNN sur plusieurs époques (par exemple, 100 époques).\n",
    "À chaque époque, passer par toutes les séquences de texte, calculer la perte et ajuster les poids.\n",
    "- **Pourquoi ?**\n",
    "L'entraînement permet au RNN d'améliorer ses prédictions en ajustant progressivement ses paramètres (poids et biais). Chaque époque aide le modèle à mieux comprendre la structure du texte pour générer des séquences plus cohérentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rnn(inputs, targets, vocab_size, one_hot_dict, n_epochs, learning_rate):\n",
    "    W_xh, W_hh, W_hy, b_h, b_y = initialize_parameters(vocab_size)  # Initialise les paramètres\n",
    "    for epoch in range(n_epochs):\n",
    "        loss = 0  # Pour suivre la perte de chaque époque\n",
    "        hprev = np.zeros((hidden_size, 1))  # Réinitialise l'état caché pour chaque époque\n",
    "        \n",
    "        for i in range(len(inputs)):\n",
    "            # Propagation avant\n",
    "            outputs, hs = forward_pass(inputs[i], hprev, W_xh, W_hh, W_hy, b_h, b_y, vocab_size, one_hot_dict)\n",
    "            \n",
    "            # Calcul de la perte (entropie croisée)\n",
    "            target_idx = char_to_idx[targets[i]]\n",
    "            loss += -np.log(outputs[-1][target_idx])\n",
    "            \n",
    "            # Rétropropagation\n",
    "            W_xh, W_hh, W_hy, b_h, b_y = backward_pass(inputs[i], targets[i], outputs, hs, W_xh, W_hh, W_hy, b_h, b_y, vocab_size, one_hot_dict, learning_rate)\n",
    "        \n",
    "        # Affichage de la perte à chaque époque\n",
    "        print(f'Époque {epoch+1}/{n_epochs}, Perte : {loss}')\n",
    "    \n",
    "    return W_xh, W_hh, W_hy, b_h, b_y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Génération de texte\n",
    "Une fois que le modèle est entraîné, tu peux l'utiliser pour générer du texte à partir d'un caractère initial.\n",
    "\n",
    "Objectif : Générer une séquence de texte de longueur 𝐿\n",
    "L en partant d'un caractère initial et en utilisant les sorties du RNN pour prédire le caractère suivant.\n",
    "\n",
    "Pourquoi ?\n",
    "La génération de texte permet de tester la capacité du modèle à apprendre des dépendances et à prédire des caractères cohérents à partir du texte appris.\n",
    "\n",
    "Comment ?\n",
    "Initialiser l'état caché avec un vecteur nul ou un état caché appris.\n",
    "Donner un caractère initial comme entrée.\n",
    "À chaque étape :\n",
    "Prédire le prochain caractère en fonction de l'état caché.\n",
    "Utiliser ce caractère comme entrée pour la prédiction suivante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(start_char, length, W_xh, W_hh, W_hy, b_h, b_y, one_hot_dict, idx_to_char):\n",
    "    hprev = np.zeros((hidden_size, 1))  # Initialisation de l'état caché\n",
    "    generated_text = start_char  # Commencer avec le caractère initial\n",
    "    \n",
    "    for i in range(length):\n",
    "        x_t = np.array(one_hot_dict[start_char]).reshape(-1, 1)  # Encoder le caractère initial\n",
    "        h = np.tanh(np.dot(W_xh, x_t) + np.dot(W_hh, hprev) + b_h)  # Propagation avant\n",
    "        \n",
    "        # Calcul de la sortie et application de softmax\n",
    "        y_raw = np.dot(W_hy, h) + b_y\n",
    "        y = np.exp(y_raw) / np.sum(np.exp(y_raw))  # Softmax\n",
    "        \n",
    "        # Choisir le caractère suivant (maximum de la probabilité)\n",
    "        next_char_idx = np.argmax(y)\n",
    "        next_char = idx_to_char[next_char_idx]\n",
    "        \n",
    "        # Ajouter le caractère à la séquence générée\n",
    "        generated_text += next_char\n",
    "        start_char = next_char  # Utiliser le prochain caractère comme entrée pour la suivante génération\n",
    "        hprev = h  # Mettre à jour l'état caché\n",
    "    \n",
    "    return generated_text\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
