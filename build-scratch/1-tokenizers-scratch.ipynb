{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizers\n",
    "\n",
    "## 64. What types of tokenizers do you know? Compare them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizers par mots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Once', 'upon', 'a', 'time,', 'Anna', 'misplaced', 'her', 'key.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Tokenisers par espaces blancs\n",
    "#--------------------------------\n",
    "texte = \"Once upon a time, Anna misplaced her key.\"\n",
    "texte.split()\n",
    "# avantages \n",
    "# * simple √† utiliser\n",
    "# * rapide\n",
    "# inconv√©nients\n",
    "# * key. et key representerons deux mots differents\n",
    "# * ne fonctionne pas si vous avez des mots avec des espaces blancs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['once', 'upon', 'a', 'time', 'anna', 'misplaced', 'her', 'key']\n"
     ]
    }
   ],
   "source": [
    "## Tokenisers par regex\n",
    "# https://www.regular-expressions.info/python.html\n",
    "import re\n",
    "text = \"Once upon a time, Anna misplaced her key.\"\n",
    "words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "print(words) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Once', 'upon', 'a', 'time', 'in', '1900', ',', 'Anna', 'misplaced', 'her', 'key', '.', '*', '#', 'tariq.ch']\n"
     ]
    }
   ],
   "source": [
    "## Tokenisers de NLTK\n",
    "# ---------------------------------------------------\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize, TreebankWordTokenizer, TweetTokenizer\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('punkt_tab')\n",
    "\n",
    "texte = \"Once upon a time in 1900, Anna misplaced her key. *#tariq.ch\"\n",
    "print(word_tokenize(texte))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Once', 'upon', 'a', 'time', 'in', '1900', ',', 'Anna', 'misplaced', 'her', 'key', '.', '*#', 'tariq', '.', 'ch', ',', 'üòéüòµ']\n"
     ]
    }
   ],
   "source": [
    "## tokenisers bas√© sur la ponctuation\n",
    "texte = \"Once upon a time in 1900, Anna misplaced her key. *#tariq.ch, üòéüòµ\"\n",
    "print(wordpunct_tokenize(texte))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Once',\n",
       " 'upon',\n",
       " 'a',\n",
       " 'time',\n",
       " 'in',\n",
       " '1900',\n",
       " ',',\n",
       " 'Anna',\n",
       " 'did',\n",
       " \"n't\",\n",
       " 'misplaced',\n",
       " 'her',\n",
       " 'key.',\n",
       " '*',\n",
       " '#',\n",
       " 'tariq.ch',\n",
       " 'üòéüòµ']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## tokenisers TreeBank Word\n",
    "# Ce tokenizer int√®gre une vari√©t√© de r√®gles communes pour la tokenisation des mots anglais.\n",
    "#  Il s√©pare la ponctuation de fin de phrase comme (?!.;,) des tokens adjacents et conserve\n",
    "# les nombres d√©cimaux comme un seul token. En outre, il contient des r√®gles pour les contractions anglaises. \n",
    "# Par exemple, ¬´ don't ¬ª est symbolis√© par [¬´ do ¬ª, ¬´ n't ¬ª]\n",
    "texte = \"Once upon a time in 1900, Anna didn't misplaced her key. *#tariq.ch üòéüòµ\"\n",
    "tokenisers = TreebankWordTokenizer()\n",
    "tokenisers.tokenize(texte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Once',\n",
       " 'upon',\n",
       " 'a',\n",
       " 'time',\n",
       " 'in',\n",
       " '1900',\n",
       " ',',\n",
       " 'Anna',\n",
       " \"didn't\",\n",
       " 'misplaced',\n",
       " 'her',\n",
       " 'key',\n",
       " '.',\n",
       " '*',\n",
       " '#tariq',\n",
       " '.',\n",
       " 'ch',\n",
       " 'üòé',\n",
       " 'üòµ']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## tokenizers par tweet\n",
    "tokenisers = TweetTokenizer()\n",
    "tokenisers.tokenize(texte)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenisers par sous mots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---> r√©sout le probl√®me de OUT OF VOCABULARY OOV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Vocabulary: Counter({'l o w </w>': 1, 'l o w e s t </w>': 1, 'n e w </w>': 1, 'n e w e s t </w>': 1})\n",
      "Step 1: Merging ('l', 'o')\n",
      "Updated Vocabulary: {'lo w </w>': 1, 'lo w e s t </w>': 1, 'n e w </w>': 1, 'n e w e s t </w>': 1}\n",
      "Step 2: Merging ('lo', 'w')\n",
      "Updated Vocabulary: {'low </w>': 1, 'low e s t </w>': 1, 'n e w </w>': 1, 'n e w e s t </w>': 1}\n",
      "Step 3: Merging ('e', 's')\n",
      "Updated Vocabulary: {'low </w>': 1, 'low es t </w>': 1, 'n e w </w>': 1, 'n e w es t </w>': 1}\n",
      "Step 4: Merging ('es', 't')\n",
      "Updated Vocabulary: {'low </w>': 1, 'low est </w>': 1, 'n e w </w>': 1, 'n e w est </w>': 1}\n",
      "Step 5: Merging ('est', '</w>')\n",
      "Updated Vocabulary: {'low </w>': 1, 'low est</w>': 1, 'n e w </w>': 1, 'n e w est</w>': 1}\n",
      "Step 6: Merging ('n', 'e')\n",
      "Updated Vocabulary: {'low </w>': 1, 'low est</w>': 1, 'ne w </w>': 1, 'ne w est</w>': 1}\n",
      "Step 7: Merging ('ne', 'w')\n",
      "Updated Vocabulary: {'low </w>': 1, 'low est</w>': 1, 'new </w>': 1, 'new est</w>': 1}\n",
      "Step 8: Merging ('low', '</w>')\n",
      "Updated Vocabulary: {'low</w>': 1, 'low est</w>': 1, 'new </w>': 1, 'new est</w>': 1}\n",
      "Step 9: Merging ('low', 'est</w>')\n",
      "Updated Vocabulary: {'low</w>': 1, 'lowest</w>': 1, 'new </w>': 1, 'new est</w>': 1}\n",
      "Step 10: Merging ('new', '</w>')\n",
      "Updated Vocabulary: {'low</w>': 1, 'lowest</w>': 1, 'new</w>': 1, 'new est</w>': 1}\n",
      "\n",
      "Final Vocabulary: {'low</w>': 1, 'lowest</w>': 1, 'new</w>': 1, 'new est</w>': 1}\n"
     ]
    }
   ],
   "source": [
    "## BPE (Byte Pair Encoding)\n",
    "# L'id√©e principale est de commencer par d√©couper chaque mot en caract√®res uniques, \n",
    "# puis de combiner it√©rativement les paires de caract√®res les plus fr√©quentes pour former des sous-mots.\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "def get_vocab(corpus):\n",
    "    \"\"\"\n",
    "    Create a vocabulary where each word is split into characters with a special token '</w>'.\n",
    "    \"\"\"\n",
    "    vocab = Counter()\n",
    "    for word in corpus:\n",
    "        word = \" \".join(list(word)) + \" </w>\"  # Split word into characters and add end-of-word token\n",
    "        vocab[word] += 1\n",
    "    return vocab\n",
    "\n",
    "def get_stats(vocab):\n",
    "    \"\"\"\n",
    "    Count frequency of character pairs in the vocabulary.\n",
    "    \"\"\"\n",
    "    pairs = defaultdict(int)\n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols) - 1):\n",
    "            pairs[symbols[i], symbols[i + 1]] += freq\n",
    "    return pairs\n",
    "\n",
    "def merge_vocab(pair, vocab):\n",
    "    \"\"\"\n",
    "    Merge the most frequent pair in the vocabulary.\n",
    "    \"\"\"\n",
    "    new_vocab = {}\n",
    "    bigram = \" \".join(pair)\n",
    "    replacement = \"\".join(pair)  # Merge the pair into one token\n",
    "    for word in vocab:\n",
    "        # Replace the pair with the merged token\n",
    "        new_word = word.replace(bigram, replacement)\n",
    "        new_vocab[new_word] = vocab[word]\n",
    "    return new_vocab\n",
    "\n",
    "def byte_pair_encoding(corpus, num_merges):\n",
    "    \"\"\"\n",
    "    Apply BPE to the corpus for a specified number of merges.\n",
    "    \"\"\"\n",
    "    vocab = get_vocab(corpus)\n",
    "    print(\"Initial Vocabulary:\", vocab)\n",
    "    for i in range(num_merges):\n",
    "        pairs = get_stats(vocab)\n",
    "        if not pairs:\n",
    "            break\n",
    "        # Find the most frequent pair\n",
    "        best_pair = max(pairs, key=pairs.get)\n",
    "        print(f\"Step {i + 1}: Merging {best_pair}\")\n",
    "        vocab = merge_vocab(best_pair, vocab)\n",
    "        print(\"Updated Vocabulary:\", vocab)\n",
    "    return vocab\n",
    "\n",
    "# Example usage\n",
    "corpus = [\"low\", \"lowest\", \"new\", \"newest\"]\n",
    "num_merges = 10\n",
    "final_vocab = byte_pair_encoding(corpus, num_merges)\n",
    "\n",
    "print(\"\\nFinal Vocabulary:\", final_vocab)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## WordPiece algorithm (used in BERT)\n",
    "# The WordPiece algorithm is a subword tokenization algorithm that was used in BERT. It was\n",
    "# designed to handle out-of-vocabulary (OOV) words by splitting them into subwords.\n",
    "# The WordPiece algorithm works as follows:\n",
    "# * 1.  Initialize a vocabulary of size V, where V is a hyperparameter that controls the size\n",
    "# * of the vocabulary.\n",
    "# * 2.  Initialize an empty vocabulary set.\n",
    "# * 3.  For each word in the training data, calculate its frequency.\n",
    "# * 4.  Sort the words by frequency in descending order.\n",
    "# * 5.  For each word, calculate its subword representation using a greedy algorithm. what is greedy algorithm ?\n",
    "# * 6.  Add the subword representation to the vocabulary set.\n",
    "# * 7.  Repeat steps 3-6 until the vocabulary set reaches the desired size V.\n",
    "# The WordPiece algorithm has several advantages, including:\n",
    "# *   It can handle OOV words by splitting them into subwords.\n",
    "# *   It can learn subword representations that are useful for language modeling.\n",
    "# *   It can be used with a wide range of languages.\n",
    "# However, the WordPiece algorithm also has some disadvantages, including:\n",
    "# *   It can be computationally expensive to train.\n",
    "# *   It requires a large amount of training data to learn effective subword representations.\n",
    "# *   It can be difficult to tune the hyperparameters of the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "65. Can you extend a tokenizer? If yes, in what case would you do this? When would you retrain a tokenizer? What needs to be done when adding new tokens?\n",
    "\n",
    "66. How do regular tokens differ from special tokens?\n",
    "\n",
    "67. Why is lemmatization not used in transformers? And why do we need tokens?\n",
    "\n",
    "68. How is a tokenizer trained? Explain with examples of WordPiece and BPE .\n",
    "\n",
    "69. What position does the CLS vector occupy? Why?\n",
    "\n",
    "70. What tokenizer is used in BERT, and which one in GPT?\n",
    "\n",
    "71. Explain how modern tokenizers handle out-of-vocabulary words?\n",
    "\n",
    "72. What does the tokenizer vocab size affect? How will you choose it in the case of new training?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
